{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b21bdd-5cf4-4f36-af76-a854abd4941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo07010/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, torch, cudaq\n",
    "import openfermion, openfermionpyscf\n",
    "import cudaq\n",
    "import numpy as np\n",
    "from pyscf import gto, scf, mcscf, ao2mo, fci, cc\n",
    "import openfermion\n",
    "from openfermion import MolecularData, get_fermion_operator, jordan_wigner\n",
    "from openfermionpyscf import run_pyscf\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--mpi', action='store_true')\n",
    "args, _ = parser.parse_known_args()       \n",
    "\n",
    "# ---- MPI / target Ë®≠ÂÆö ----\n",
    "rank = 0\n",
    "world = 1\n",
    "if args.mpi:\n",
    "    try:\n",
    "        cudaq.set_target('nvidia', option='mqpu')\n",
    "        if hasattr(cudaq, 'mpi') and not cudaq.mpi.is_initialized():\n",
    "            cudaq.mpi.initialize()\n",
    "        rank  = cudaq.mpi.rank()\n",
    "        world = cudaq.mpi.num_ranks()\n",
    "    except RuntimeError as e:\n",
    "        print(f'Warning: {e}\\nFalling back to single-process CPU/QPU...')\n",
    "        args.mpi = False\n",
    "\n",
    "if not args.mpi:\n",
    "    try:\n",
    "        cudaq.set_target('nvidia', option='fp64')\n",
    "    except RuntimeError:\n",
    "        cudaq.set_target('qpp-cpu')\n",
    "\n",
    "import cudaq_solvers as solvers\n",
    "from cudaq import spin\n",
    "from lightning.fabric.loggers import CSVLogger\n",
    "from cudaq_solvers.gqe_algorithm.gqe import get_default_config\n",
    "# ÂÜçÁèæÊÄßË®≠ÂÆö\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "torch.manual_seed(3047)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3f0142f4-b761-496a-ae6d-eecd242f9eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating N2 with Active Space: (10, 7) (n_elec, n_orb)\n",
      "Ëá™ÂãïË®≠ÂÆö Active Space Indices: [2, 3, 4, 5, 6, 7, 8] (Core orbitals: 0-1)\n",
      "\n",
      "=== N2 Energy Summary ===\n",
      "Full HF Energy      = -107.49589331 Ha\n",
      "Full FCI Energy     = -107.65282873 Ha\n",
      "Full CCSD Energy    = -107.64894129 Ha\n",
      "Active Const Offset = -76.40914089 Ha (Nuclear + Frozen Core)\n",
      "\n",
      "=== Hamiltonian Summary ===\n",
      "Number of Qubits    : 14\n",
      "Number of Electrons : 10\n",
      "Hamiltonian Terms   : 670\n",
      "\n",
      "=== Tensor Shapes ===\n",
      "h_pq shape          : (7, 7)\n",
      "g_pqrs shape        : (7, 7, 7, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2519516/1268176381.py:184: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  spin_ham = cudaq.SpinOperator(qubit_ham_of)\n"
     ]
    }
   ],
   "source": [
    "MOLECULE_DATA = {\n",
    "    \"H2\": {\n",
    "        \"geometry\": [(\"H\", (0.0, 0.0, 0.0)), (\"H\", (0.0, 0.0, 0.7414))],\n",
    "        \"basis\": \"sto-3g\",\n",
    "        \"multiplicity\": 1,\n",
    "        \"charge\": 0\n",
    "    },\n",
    "    \"N2\": {\n",
    "        \"geometry\": [(\"N\", (0.0, 0.0, 0.0)), (\"N\", (0.0, 0.0, 1.0977))],\n",
    "        \"basis\": \"sto-3g\",\n",
    "        \"multiplicity\": 1,\n",
    "        \"charge\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 1. ÂÆöÁæ© get_pyscf_results ÂáΩÂºè\n",
    "# ==========================================\n",
    "def get_pyscf_results(molecule_name, dist_scale=1.0, active_space=None):\n",
    "    \"\"\"\n",
    "    Âª∫Á´ãÂàÜÂ≠êÔºåÂü∑Ë°å HF/FCI/CCSD/CCSD(T)Ôºå‰∏¶ÁÇ∫ÊåáÂÆöÁöÑÊ¥ªË∫çÁ©∫ÈñìÂª∫ÊßãÁ©çÂàÜÂíåÈáèÂ≠ê‰ΩçÂÖÉÂìàÂØÜÈ†ìÈáè„ÄÇ\n",
    "    \n",
    "    Args:\n",
    "        molecule_name (str): ÂàÜÂ≠êÂêçÁ®± (‰æÜËá™ MOLECULE_DATA)„ÄÇ\n",
    "        dist_scale (float): ÈõôÂéüÂ≠êÂàÜÂ≠êÁöÑÈçµÈï∑ÊØî‰æã„ÄÇ\n",
    "        active_space (list[int] | tuple[int, int] | None): \n",
    "            - Â¶ÇÊûúÊòØ list[int]: ÊåáÂÆöË¶Å‰ΩøÁî®ÁöÑÊ¥ªË∫çÁ©∫ÈñìËªåÂüüÁ¥¢ÂºïÂàóË°® (‰æãÂ¶Ç [2,3,4])„ÄÇ\n",
    "            - Â¶ÇÊûúÊòØ tuple[int, int]: ÊåáÂÆö (Ê¥ªË∫çÈõªÂ≠êÊï∏, Ê¥ªË∫çËªåÂüüÊï∏)Ôºå‰æãÂ¶Ç (10, 7)„ÄÇÁ®ãÂºèÊúÉËá™ÂãïË®àÁÆóÊ†∏ÂøÉËªåÂüü‰∏¶ÈÅ∏ÂèñÂçÄÈñì„ÄÇ\n",
    "            - Â¶ÇÊûúÁÇ∫ None: ‰ΩøÁî®ÊâÄÊúâËªåÂüü„ÄÇ\n",
    "\n",
    "    Returns:\n",
    "        tuple: ÂåÖÂê´ PySCF ÂàÜÂ≠êÁâ©‰ª∂„ÄÅÂêÑÁ®ÆËÉΩÈáè„ÄÅÂìàÂØÜÈ†ìÈáèÁÆóÁ¨¶ÂèäÁ©çÂàÜÂºµÈáèÁ≠âË≥áË®ä„ÄÇ\n",
    "    \"\"\"\n",
    "    if molecule_name not in MOLECULE_DATA:\n",
    "        raise ValueError(f\"Molecule {molecule_name} not found in MOLECULE_DATA.\")\n",
    "\n",
    "    data = MOLECULE_DATA[molecule_name]\n",
    "    geom = data['geometry']\n",
    "\n",
    "    # ËôïÁêÜÈçµÈï∑Á∏ÆÊîæ\n",
    "    if len(geom) == 2 and dist_scale != 1.0:\n",
    "        atom1, pos1 = geom[0]\n",
    "        atom2, pos2 = geom[1]\n",
    "        scaled_pos2 = tuple([p * dist_scale for p in pos2])\n",
    "        scaled_geom = [[atom1, pos1], [atom2, scaled_pos2]]\n",
    "    else:\n",
    "        scaled_geom = geom\n",
    "\n",
    "    # --- PySCF ÂàùÂßãÂåñ ---\n",
    "    mol = gto.Mole()\n",
    "    mol.atom = scaled_geom\n",
    "    mol.basis = data['basis']\n",
    "    mol.charge = data['charge']\n",
    "    mol.spin = data['multiplicity'] - 1\n",
    "    mol.build(verbose=0)\n",
    "\n",
    "    # --- Âü∑Ë°å Hartree-Fock ---\n",
    "    mf = scf.RHF(mol).run(verbose=0)\n",
    "    hf_e = mf.e_tot\n",
    "\n",
    "    # --- üíé Ë®àÁÆóÊ¥ªË∫çÁ©∫ÈñìÁ©çÂàÜ (Active Space Integrals) ---\n",
    "    num_mos_total = mf.mo_coeff.shape[1]\n",
    "\n",
    "    # Âà§Êñ∑ active_space È°ûÂûã‰∏¶Ë®≠ÂÆö current_active_space (Á¥¢ÂºïÂàóË°®)\n",
    "    if active_space is None:\n",
    "        # None: ÂÖ®Á©∫Èñì\n",
    "        current_active_space = list(range(num_mos_total))\n",
    "        \n",
    "    elif isinstance(active_space, tuple) and len(active_space) == 2:\n",
    "        # Tuple (nelec_cas, norb_cas): Ëá™ÂãïË®àÁÆóÁ¥¢Âºï\n",
    "        n_cas_elec, n_cas_orb = active_space\n",
    "        \n",
    "        n_total_elec = int(sum(mf.mo_occ))\n",
    "        n_core_elec = n_total_elec - n_cas_elec\n",
    "        \n",
    "        if n_core_elec < 0:\n",
    "             raise ValueError(f\"Ë®≠ÂÆöÁöÑÊ¥ªË∫çÈõªÂ≠êÊï∏ ({n_cas_elec}) Ë∂ÖÈÅéÁ∏ΩÈõªÂ≠êÊï∏ ({n_total_elec})\")\n",
    "             \n",
    "        n_core_orb = n_core_elec // 2  # ÂÅáË®≠Ê†∏ÂøÉËªåÂüüÈÉΩÊòØÈõô‰ΩîÊìö\n",
    "        \n",
    "        # Ê™¢Êü•ÁØÑÂúçÊòØÂê¶ÂêàÊ≥ï\n",
    "        if n_core_orb + n_cas_orb > num_mos_total:\n",
    "             raise ValueError(f\"ÈúÄË¶ÅÁöÑËªåÂüüÊï∏ (Core {n_core_orb} + Active {n_cas_orb}) Ë∂ÖÈÅéÁ∏ΩËªåÂüüÊï∏ ({num_mos_total})\")\n",
    "             \n",
    "        # Ëá™ÂãïÁî¢ÁîüÁ¥¢ÂºïÔºöÂæûÊ†∏ÂøÉËªåÂüü‰πãÂæåÈñãÂßãÁÆó n_cas_orb ÂÄã\n",
    "        current_active_space = list(range(n_core_orb, n_core_orb + n_cas_orb))\n",
    "        print(f\"Ëá™ÂãïË®≠ÂÆö Active Space Indices: {current_active_space} (Core orbitals: 0-{n_core_orb-1})\")\n",
    "        \n",
    "    else:\n",
    "        # List: Áõ¥Êé•‰ΩøÁî®‰ΩøÁî®ËÄÖÊèê‰æõÁöÑÁ¥¢Âºï\n",
    "        current_active_space = active_space\n",
    "\n",
    "    # 1. num_orbitals\n",
    "    num_orbitals = len(current_active_space)\n",
    "\n",
    "    # 2. n_electrons (Ê¥ªË∫çÁ©∫ÈñìÂÖß)\n",
    "    n_electrons = int(sum(mf.mo_occ[current_active_space]))\n",
    "    num_elec_a = (n_electrons + mol.spin) // 2\n",
    "    num_elec_b = (n_electrons - mol.spin) // 2\n",
    "    nelec = (num_elec_a, num_elec_b)\n",
    "\n",
    "    # 3. cas\n",
    "    cas = mcscf.CASCI(mf, num_orbitals, nelec)\n",
    "    \n",
    "    # 4. mo\n",
    "    mo = cas.sort_mo(current_active_space, base=0)\n",
    "    \n",
    "    # 5. hcore, constant_offset\n",
    "    hcore, nuclear_repulsion_energy = cas.get_h1cas(mo)\n",
    "\n",
    "    # 6. eri\n",
    "    eri_cas = cas.get_h2eff(mo)\n",
    "    eri = ao2mo.restore(1, eri_cas, num_orbitals)\n",
    "\n",
    "    # --- ÂÇ≥Áµ±ÊñπÊ≥ïË®àÁÆó (FCI & CCSD) ---\n",
    "    try:\n",
    "        fci_solver = fci.FCI(mf)\n",
    "        fci_e, _ = fci_solver.kernel()\n",
    "    except Exception:\n",
    "        fci_e = hf_e\n",
    "\n",
    "    try:\n",
    "        ccsd_solver = cc.CCSD(mf)\n",
    "        ccsd_solver.kernel()\n",
    "        ccsd_e = ccsd_solver.e_tot\n",
    "        ccsd_t_e = ccsd_e + ccsd_solver.ccsd_t()\n",
    "    except Exception:\n",
    "        ccsd_e = hf_e\n",
    "        ccsd_t_e = hf_e\n",
    "\n",
    "    # --- OpenFermion ÈÉ®ÂàÜ ---\n",
    "    mol_of = MolecularData(\n",
    "        geometry=scaled_geom,\n",
    "        basis=data['basis'],\n",
    "        multiplicity=data['multiplicity'],\n",
    "        charge=data['charge']\n",
    "    )\n",
    "    mol_of = run_pyscf(mol_of, run_scf=True, run_fci=False, verbose=False)\n",
    "    \n",
    "    all_indices = set(range(num_mos_total))\n",
    "    active_set = set(current_active_space)\n",
    "    core_indices = [i for i in range(num_mos_total) if i not in active_set and mf.mo_occ[i] > 0.0]\n",
    "\n",
    "    fermion_ham = mol_of.get_molecular_hamiltonian(\n",
    "        occupied_indices=core_indices,\n",
    "        active_indices=current_active_space\n",
    "    )\n",
    "    \n",
    "    fermion_op = get_fermion_operator(fermion_ham)\n",
    "    qubit_ham = jordan_wigner(fermion_op)\n",
    "\n",
    "    return (\n",
    "        mol, hf_e, fci_e, ccsd_e, ccsd_t_e, qubit_ham,\n",
    "        hcore, eri, nuclear_repulsion_energy, num_orbitals, nelec\n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# 2. ‰∏ªÁ®ãÂºèÂü∑Ë°åÂçÄÂ°ä\n",
    "# ==========================================\n",
    "# ÈÅ∏ÊìáÂàÜÂ≠ê\n",
    "target_molecule = \"N2\" \n",
    "# target_molecule = \"H2\" # ÂàáÊèõÈÄôÂÄãÂèØ‰ª•Ê∏¨Ë©¶ H2\n",
    "\n",
    "if target_molecule == \"N2\":\n",
    "    # N2 Ë®≠ÂÆö: (10 ÈõªÂ≠ê, 7 ËªåÂüü)\n",
    "    # ‰ΩøÁî® tuple Ê†ºÂºè (nele_cas, norb_cas)\n",
    "    active_space_input = (10, 7) \n",
    "elif target_molecule == \"H2\":\n",
    "    # H2 Ë®≠ÂÆö: (2 ÈõªÂ≠ê, 2 ËªåÂüü) -> ÂÖ®Á©∫Èñì\n",
    "    active_space_input = None\n",
    "else:\n",
    "    active_space_input = None\n",
    "\n",
    "print(f\"Calculating {target_molecule} with Active Space: {active_space_input} (n_elec, n_orb)\")\n",
    "\n",
    "# ÂëºÂè´ÂáΩÂºè\n",
    "(mol, hf_e, fci_e, ccsd_e, ccsd_t_e, qubit_ham_of, \n",
    " hcore, eri, active_const, norb_cas, nele_cas_tuple) = get_pyscf_results(\n",
    "    molecule_name=target_molecule,\n",
    "    active_space=active_space_input\n",
    ")\n",
    "\n",
    "# ËΩâÊèõÁÇ∫ CUDA-Q SpinOperator\n",
    "spin_ham = cudaq.SpinOperator(qubit_ham_of)\n",
    "\n",
    "# Ë®≠ÂÆöÂæåÁ∫åÈúÄË¶ÅÁöÑÂÖ®ÂüüËÆäÊï∏\n",
    "n_qubits = norb_cas * 2\n",
    "n_electrons = sum(nele_cas_tuple)\n",
    "\n",
    "print(f\"\\n=== {target_molecule} Energy Summary ===\")\n",
    "print(f\"Full HF Energy      = {hf_e:.8f} Ha\")\n",
    "print(f\"Full FCI Energy     = {fci_e:.8f} Ha\")\n",
    "print(f\"Full CCSD Energy    = {ccsd_e:.8f} Ha\")\n",
    "print(f\"Active Const Offset = {active_const:.8f} Ha (Nuclear + Frozen Core)\")\n",
    "\n",
    "print(\"\\n=== Hamiltonian Summary ===\")\n",
    "print(f\"Number of Qubits    : {n_qubits}\")\n",
    "print(f\"Number of Electrons : {n_electrons}\")\n",
    "print(f\"Hamiltonian Terms   : {spin_ham.term_count}\")\n",
    "\n",
    "print(\"\\n=== Tensor Shapes ===\")\n",
    "print(f\"h_pq shape          : {hcore.shape}\")\n",
    "print(f\"g_pqrs shape        : {eri.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bc82f242-08ee-4bc9-87d5-7280dcff6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geometry = [(\"H\", (0.0, 0.0, 0.0)), (\"H\", (0.0, 0.0, 1.0977))]\n",
    "\n",
    "# basis = 'sto3g'\n",
    "# multiplicity = 1\n",
    "# charge = 0\n",
    "# ncore = 0\n",
    "# nele_cas, norb_cas = (2, 2)\n",
    "\n",
    "# molecule = openfermion.MolecularData(geometry, basis, multiplicity, charge)\n",
    "# molecule = openfermionpyscf.run_pyscf(\n",
    "#     molecule,\n",
    "#     run_scf=1,\n",
    "#     run_ccsd=1,\n",
    "#     run_fci=1,  \n",
    "# )\n",
    "\n",
    "\n",
    "# molecular_hamiltonian = molecule.get_molecular_hamiltonian(\n",
    "#     occupied_indices=range(ncore),\n",
    "#     active_indices=range(ncore, ncore + norb_cas))\n",
    "\n",
    "# fermion_hamiltonian = openfermion.get_fermion_operator(molecular_hamiltonian)\n",
    "\n",
    "# qubit_hamiltonian = openfermion.jordan_wigner(fermion_hamiltonian)\n",
    "\n",
    "# spin_ham = cudaq.SpinOperator(qubit_hamiltonian)\n",
    "\n",
    "# print(\"=== Energy Summary ===\")\n",
    "# print(f\"Full HF Energy      = {molecule.hf_energy:.8f} Ha\")\n",
    "# print(f\"Full FCI Energy     = {molecule.fci_energy:.8f} Ha\")\n",
    "\n",
    "# active_const = molecular_hamiltonian.constant     \n",
    "# full_fci = molecule.fci_energy                   \n",
    "# frozen_core_energy = full_fci - active_const\n",
    "# print(f\"Frozen-core energy contribution = {frozen_core_energy:.8f} Ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "344cb765-4500-45a3-85b4-2619116621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the molecular hamiltonian\n",
    "# geometry = [('Li', (0., 0., 0.)), ('H', (0., 0., .7474))]\n",
    "# molecule = solvers.create_molecule(geometry, 'sto-3g', 0, 0, casci=True)\n",
    "\n",
    "# spin_ham = molecule.hamiltonian\n",
    "# n_qubits = molecule.n_orbitals * 2\n",
    "# n_electrons = molecule.n_electrons\n",
    "# print(molecule.energies)  \n",
    "# print('n_qubits=',n_qubits)\n",
    "# for term in spin_ham:\n",
    "#     coeff = term.evaluate_coefficient()\n",
    "#     pauli = term.get_pauli_word(n_qubits)\n",
    "#     print(f\"{coeff:.6f} * {pauli}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4341ad96-12b9-4257-a18c-2ff9f7f18460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice: get_identity returned an empty operator (Zero) and was skipped.\n",
      "Total operators in pool = 1400\n"
     ]
    }
   ],
   "source": [
    "import cudaq\n",
    "import numpy as np\n",
    "from cudaq import SpinOperator\n",
    "\n",
    "# Ê≥®ÊÑèÔºöSpinOperator() Âª∫Á´ãÁöÑÊòØ \"Zero\" (Á©∫)Ôºå‰∏çÊòØ Identity„ÄÇ\n",
    "# Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶Å IdentityÔºåÈÄöÂ∏∏ÈúÄË¶ÅÁî® from_word ÊàñËÄÖÈ°û‰ººÊñπÊ≥ïÔºå‰ΩÜÈÄôË£°ÊàëÂÄëÂÖà‰øùÁïô‰Ω†ÁöÑÈÇèËºØ‰∏¶Âä†‰∏äÈÅéÊøæ„ÄÇ\n",
    "def get_identity(n_qubits: int) -> SpinOperator:\n",
    "    return SpinOperator() \n",
    "\n",
    "pool = []\n",
    "params = [2**k / 320 for k in range(0, 5)] + [-2**k / 320 for k in range(0, 5)]\n",
    "operators = solvers.get_operator_pool(\"uccsd\", num_qubits=n_qubits, num_electrons=n_electrons)\n",
    "\n",
    "# ÂÆöÁæ©‰∏ÄÂÄã helper ÂáΩÊï∏‰æÜÊ™¢Êü• Operator ÊòØÂê¶ÁÇ∫Á©∫\n",
    "def is_valid_operator(op: SpinOperator) -> bool:\n",
    "    # ‰ΩøÁî® .term_count Â±¨ÊÄß (‰∏çË¶ÅÂä†Êã¨Ëôü)\n",
    "    return op.term_count > 0\n",
    "    \n",
    "    # ÊñπÊ≥ï B: ÂòóË©¶Ëø≠‰ª£ (ÊúÄÈÄöÁî®ÁöÑ‰øùÈö™ÂÅöÊ≥ï)\n",
    "    # Â¶ÇÊûú next(iter(op)) ÊããÂá∫ StopIterationÔºå‰ª£Ë°®ÂÆÉÊòØÁ©∫ÁöÑ\n",
    "    try:\n",
    "        next(iter(op))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "\n",
    "# 1. ËôïÁêÜ Identity (Êàñ Zero)\n",
    "ident = get_identity(n_qubits)\n",
    "if is_valid_operator(ident):\n",
    "    pool.append(ident)\n",
    "else:\n",
    "    print(\"Notice: get_identity returned an empty operator (Zero) and was skipped.\")\n",
    "\n",
    "# 2. ËôïÁêÜ UCCSD Operators\n",
    "for o in operators:\n",
    "    for p in params:\n",
    "        scaled_op = p * o  # Âª∫Á´ã Scaled Operator\n",
    "        \n",
    "        # --- Âú®ÈÄôË£°Âà§Êñ∑ÊääÁ©∫ÁöÑÊãøÊéâ ---\n",
    "        if is_valid_operator(scaled_op):\n",
    "            pool.append(scaled_op)\n",
    "        # -------------------------\n",
    "\n",
    "op_pool = pool\n",
    "print(f\"Total operators in pool = {len(pool)}\")\n",
    "\n",
    "# È©óË≠âÊ™¢Êü•\n",
    "# for i, op in enumerate(pool):\n",
    "#     print(f\"[{i}] {op}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0f8bd5cb-1a49-4ab1-be6a-c3c629cf3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_coefficients(op: cudaq.SpinOperator) -> list[complex]:\n",
    "    return [term.evaluate_coefficient() for term in op]\n",
    "\n",
    "\n",
    "def term_words(op: cudaq.SpinOperator) -> list[cudaq.pauli_word]:\n",
    "    return [term.get_pauli_word(n_qubits) for term in op]\n",
    "\n",
    "# Kernel that applies the selected operators\n",
    "@cudaq.kernel\n",
    "def kernel(n_qubits: int, n_electrons: int, coeffs: list[float],\n",
    "           words: list[cudaq.pauli_word]):\n",
    "    q = cudaq.qvector(n_qubits)\n",
    "\n",
    "    for i in range(n_electrons):\n",
    "        x(q[i])\n",
    "\n",
    "    for i in range(len(coeffs)):\n",
    "        exp_pauli(coeffs[i], q, words[i])\n",
    "\n",
    "\n",
    "def cost(sampled_ops: list[cudaq.SpinOperator], **kwargs):\n",
    "\n",
    "    full_coeffs = []\n",
    "    full_words = []\n",
    "\n",
    "    for op in sampled_ops:\n",
    "        full_coeffs += [c.real for c in term_coefficients(op)]\n",
    "        full_words += term_words(op)\n",
    "\n",
    "    if args.mpi:\n",
    "        handle = cudaq.observe_async(kernel,\n",
    "                                     spin_ham,\n",
    "                                     n_qubits,\n",
    "                                     n_electrons,\n",
    "                                     full_coeffs,\n",
    "                                     full_words,\n",
    "                                     qpu_id=kwargs['qpu_id'])\n",
    "        return handle, lambda res: res.get().expectation()\n",
    "    else:\n",
    "        return cudaq.observe(kernel, spin_ham, n_qubits, n_electrons,\n",
    "                             full_coeffs, full_words).expectation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "32bd84d8-6258-4956-8810-5646b555e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GQE\n",
    "cfg = get_default_config()\n",
    "cfg.use_fabric_logging = False\n",
    "cfg.max_iters = 100\n",
    "logger = CSVLogger(\"gqe_h2_logs/gqe.csv\")\n",
    "cfg.fabric_logger = logger\n",
    "cfg.save_trajectory = False \n",
    "cfg.verbose = True\n",
    "cfg.small = False\n",
    "cfg.energy_offset = 106.0\n",
    "cfg.temperature = 1\n",
    "cfg.lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9e69d32a-c07a-449d-b551-bb88209180d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 86.92M\n",
      "epoch 0 loss tensor(9.6810, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30402612686157227 tensor(-107.4915)\n",
      "epoch 1 loss tensor(6.9811, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29857563972473145 tensor(-107.4845)\n",
      "epoch 2 loss tensor(5.2610, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2995319366455078 tensor(-107.4930)\n",
      "epoch 3 loss tensor(1807.3453, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3030245304107666 tensor(-107.4474)\n",
      "epoch 4 loss tensor(4.8147, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29761767387390137 tensor(-107.5048)\n",
      "epoch 5 loss tensor(3.3717, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2969386577606201 tensor(-107.4926)\n",
      "epoch 6 loss tensor(1.8577, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29798078536987305 tensor(-107.4956)\n",
      "epoch 7 loss tensor(0.8445, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2948477268218994 tensor(-107.4919)\n",
      "epoch 8 loss tensor(37.0576, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29579687118530273 tensor(-107.4924)\n",
      "epoch 9 loss tensor(7.1620, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29961514472961426 tensor(-107.4925)\n",
      "epoch 10 loss tensor(0.6181, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3018674850463867 tensor(-107.4926)\n",
      "epoch 11 loss tensor(1.4070, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3050110340118408 tensor(-107.4883)\n",
      "epoch 12 loss tensor(0.4325, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2975766658782959 tensor(-107.4951)\n",
      "epoch 13 loss tensor(0.1932, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2994058132171631 tensor(-107.4921)\n",
      "epoch 14 loss tensor(0.7216, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3120157718658447 tensor(-107.4915)\n",
      "epoch 15 loss tensor(0.1022, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3215677738189697 tensor(-107.4831)\n",
      "epoch 16 loss tensor(1.0596, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2963695526123047 tensor(-107.5088)\n",
      "epoch 17 loss tensor(0.3201, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29737067222595215 tensor(-107.4884)\n",
      "epoch 18 loss tensor(0.6382, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29592466354370117 tensor(-107.4924)\n",
      "epoch 19 loss tensor(1.5677, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2967414855957031 tensor(-107.4821)\n",
      "epoch 20 loss tensor(0.3777, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29740118980407715 tensor(-107.4919)\n",
      "epoch 21 loss tensor(0.8814, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30277514457702637 tensor(-107.4956)\n",
      "epoch 22 loss tensor(0.2833, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3072340488433838 tensor(-107.4897)\n",
      "epoch 23 loss tensor(0.2158, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3131535053253174 tensor(-107.4871)\n",
      "epoch 24 loss tensor(0.4339, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33347630500793457 tensor(-107.4878)\n",
      "epoch 25 loss tensor(2.0435, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3146944046020508 tensor(-107.4870)\n",
      "epoch 26 loss tensor(0.2641, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30055761337280273 tensor(-107.4865)\n",
      "epoch 27 loss tensor(0.9940, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29706358909606934 tensor(-107.4990)\n",
      "epoch 28 loss tensor(0.3556, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2969684600830078 tensor(-107.5051)\n",
      "epoch 29 loss tensor(0.2892, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2985098361968994 tensor(-107.4878)\n",
      "epoch 30 loss tensor(0.8967, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.297940731048584 tensor(-107.4959)\n",
      "epoch 31 loss tensor(0.6050, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2960209846496582 tensor(-107.4925)\n",
      "epoch 32 loss tensor(0.2777, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2971048355102539 tensor(-107.4975)\n",
      "epoch 33 loss tensor(1.3220, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2985503673553467 tensor(-107.4933)\n",
      "epoch 34 loss tensor(0.3690, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3224368095397949 tensor(-107.4962)\n",
      "epoch 35 loss tensor(0.5494, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30965161323547363 tensor(-107.4943)\n",
      "epoch 36 loss tensor(0.2929, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3004286289215088 tensor(-107.4959)\n",
      "epoch 37 loss tensor(1.1294, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29663872718811035 tensor(-107.4905)\n",
      "epoch 38 loss tensor(0.1531, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29892444610595703 tensor(-107.5054)\n",
      "epoch 39 loss tensor(0.2127, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2980954647064209 tensor(-107.5028)\n",
      "epoch 40 loss tensor(0.3298, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29995131492614746 tensor(-107.4921)\n",
      "epoch 41 loss tensor(0.6684, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2973363399505615 tensor(-107.4917)\n",
      "epoch 42 loss tensor(0.1046, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2990701198577881 tensor(-107.4894)\n",
      "epoch 43 loss tensor(0.2364, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2981390953063965 tensor(-107.4981)\n",
      "epoch 44 loss tensor(0.1207, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2965433597564697 tensor(-107.4943)\n",
      "epoch 45 loss tensor(0.2483, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29622912406921387 tensor(-107.4983)\n",
      "epoch 46 loss tensor(0.1243, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2999231815338135 tensor(-107.5002)\n",
      "epoch 47 loss tensor(0.5754, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29792308807373047 tensor(-107.4963)\n",
      "epoch 48 loss tensor(0.3030, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29883360862731934 tensor(-107.5049)\n",
      "epoch 49 loss tensor(0.3772, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29925990104675293 tensor(-107.5107)\n",
      "epoch 50 loss tensor(0.2717, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2989938259124756 tensor(-107.4930)\n",
      "epoch 51 loss tensor(0.4250, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2981386184692383 tensor(-107.4940)\n",
      "epoch 52 loss tensor(0.2218, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29723620414733887 tensor(-107.4939)\n",
      "epoch 53 loss tensor(0.1020, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2960062026977539 tensor(-107.4873)\n",
      "epoch 54 loss tensor(0.1395, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29665517807006836 tensor(-107.4985)\n",
      "epoch 55 loss tensor(0.1326, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2994813919067383 tensor(-107.4884)\n",
      "epoch 56 loss tensor(0.0959, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2965853214263916 tensor(-107.4850)\n",
      "epoch 57 loss tensor(0.1173, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2965278625488281 tensor(-107.4801)\n",
      "epoch 58 loss tensor(0.1747, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2981255054473877 tensor(-107.4904)\n",
      "epoch 59 loss tensor(0.1266, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30061793327331543 tensor(-107.4912)\n",
      "epoch 60 loss tensor(0.4471, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3013882637023926 tensor(-107.4748)\n",
      "epoch 61 loss tensor(0.6993, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30307483673095703 tensor(-107.4765)\n",
      "epoch 62 loss tensor(0.1410, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2987680435180664 tensor(-107.4816)\n",
      "epoch 63 loss tensor(0.5392, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29877591133117676 tensor(-107.5029)\n",
      "epoch 64 loss tensor(0.1686, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29678988456726074 tensor(-107.4845)\n",
      "epoch 65 loss tensor(0.1382, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2966573238372803 tensor(-107.4886)\n",
      "epoch 66 loss tensor(0.1864, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29652857780456543 tensor(-107.4924)\n",
      "epoch 67 loss tensor(0.1234, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.294877290725708 tensor(-107.4899)\n",
      "epoch 68 loss tensor(0.1022, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29929566383361816 tensor(-107.4964)\n",
      "epoch 69 loss tensor(0.0939, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29697084426879883 tensor(-107.4904)\n",
      "epoch 70 loss tensor(0.5367, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30002284049987793 tensor(-107.4849)\n",
      "epoch 71 loss tensor(0.1900, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3007678985595703 tensor(-107.4934)\n",
      "epoch 72 loss tensor(0.0366, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29988861083984375 tensor(-107.4864)\n",
      "epoch 73 loss tensor(0.3053, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29912567138671875 tensor(-107.4865)\n",
      "epoch 74 loss tensor(0.0689, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2986488342285156 tensor(-107.4949)\n",
      "epoch 75 loss tensor(0.2620, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.298724889755249 tensor(-107.4921)\n",
      "epoch 76 loss tensor(0.1621, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29636645317077637 tensor(-107.4818)\n",
      "epoch 77 loss tensor(0.4801, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29822206497192383 tensor(-107.4901)\n",
      "epoch 78 loss tensor(0.1260, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29761219024658203 tensor(-107.4806)\n",
      "epoch 79 loss tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2958955764770508 tensor(-107.4855)\n",
      "epoch 80 loss tensor(0.1234, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2971987724304199 tensor(-107.4892)\n",
      "epoch 81 loss tensor(0.1723, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29883408546447754 tensor(-107.4938)\n",
      "epoch 82 loss tensor(0.4199, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2977480888366699 tensor(-107.4917)\n",
      "epoch 83 loss tensor(1.2890, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29881858825683594 tensor(-107.4801)\n",
      "epoch 84 loss tensor(0.2832, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29812049865722656 tensor(-107.4876)\n",
      "epoch 85 loss tensor(0.1047, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3039395809173584 tensor(-107.4904)\n",
      "epoch 86 loss tensor(0.1587, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29781460762023926 tensor(-107.4863)\n",
      "epoch 87 loss tensor(0.0411, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3015553951263428 tensor(-107.4940)\n",
      "epoch 88 loss tensor(0.1840, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29775094985961914 tensor(-107.4886)\n",
      "epoch 89 loss tensor(0.2806, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29691600799560547 tensor(-107.4939)\n",
      "epoch 90 loss tensor(0.3370, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.29676342010498047 tensor(-107.4891)\n",
      "epoch 91 loss tensor(0.0256, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2966008186340332 tensor(-107.4944)\n",
      "epoch 92 loss tensor(0.3605, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2988855838775635 tensor(-107.4916)\n",
      "epoch 93 loss tensor(0.0584, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.298675537109375 tensor(-107.5089)\n",
      "epoch 94 loss tensor(0.1528, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3023974895477295 tensor(-107.4862)\n",
      "epoch 95 loss tensor(0.6691, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3032679557800293 tensor(-107.4953)\n",
      "epoch 96 loss tensor(0.1439, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2992689609527588 tensor(-107.5091)\n",
      "epoch 97 loss tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3005177974700928 tensor(-107.5071)\n",
      "epoch 98 loss tensor(0.1252, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.2994215488433838 tensor(-107.4973)\n",
      "epoch 99 loss tensor(0.0373, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3023056983947754 tensor(-107.4987)\n"
     ]
    }
   ],
   "source": [
    "# Run GQE\n",
    "from GQE_tool.gqe import gqe\n",
    "minE, best_ops = gqe(cost, op_pool, max_iters=10, ngates=20, config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "08d499b3-48a5-49bc-b0aa-63760d49a5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Energy = -107.51065826416016\n",
      "Ansatz Ops\n",
      "0.0015625 IIXXIIIIIIXZZY\n",
      "-0.000390625 IIIXZZZZXIIXYI\n",
      "0.003125 IIIIIIXXIIIXYI\n",
      "-0.000390625 IIIIXXIIIIXZZY\n",
      "-0.00078125 IIIIIIIIXXXZZY\n",
      "0.003125 IIXZZZXIIIXZYI\n",
      "-0.00078125 IIIIXXIIIIIXYI\n",
      "-0.003125 IIIIIXZZXIXZZY\n",
      "0.0015625 IIIIXZZXIIXZZY\n",
      "0.003125 IIIIXZXIIIXZYI\n",
      "0.003125 IIIXZZZZXIIIXY\n",
      "-0.025 IIIIIYZZZZZXII\n",
      "0.003125 IIIIXZXIIIXZYI\n",
      "-0.000390625 IIIXZZZZXIIXYI\n",
      "0.003125 IIIIXZXIIIXZYI\n",
      "0.00078125 XZZZZXIIIIXZZY\n",
      "0.00078125 IIXZZZXIIIXZYI\n",
      "0.003125 IXZZZZXIIIIXYI\n",
      "-0.000390625 IIIIIIXZZXXYII\n",
      "0.003125 IIIIXZXIIIXZYI\n"
     ]
    }
   ],
   "source": [
    "# Only print results from rank 0 when using MPI\n",
    "if not args.mpi or cudaq.mpi.rank() == 0:\n",
    "    print(f'Ground Energy = {minE}')\n",
    "    print('Ansatz Ops')\n",
    "    for idx in best_ops:\n",
    "        # Get the first (and only) term since these are simple operators\n",
    "        term = next(iter(op_pool[idx]))\n",
    "        print(term.evaluate_coefficient().real, term.get_pauli_word(n_qubits))\n",
    "\n",
    "if args.mpi:\n",
    "    cudaq.mpi.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fea764e3-c7bf-4737-837a-cf271466b08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Sweep for lr ===\n",
      "\n",
      "Running with lr = 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 86.92M\n",
      "epoch 0 loss tensor(0.4556, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3257558345794678 tensor(-107.4851)\n",
      "epoch 1 loss tensor(2078.2605, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31892991065979004 tensor(-107.4298)\n",
      "epoch 2 loss tensor(0.3691, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3127734661102295 tensor(-107.4891)\n",
      "epoch 3 loss tensor(1.8167e+09, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3182506561279297 tensor(-107.3593)\n",
      "epoch 4 loss tensor(267364.9688, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30689311027526855 tensor(-107.4937)\n",
      "epoch 5 loss tensor(5346.1709, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3176579475402832 tensor(-107.3036)\n",
      "epoch 6 loss tensor(461.8869, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3103804588317871 tensor(-107.4768)\n",
      "epoch 7 loss tensor(27.1714, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31480979919433594 tensor(-107.4712)\n",
      "epoch 8 loss tensor(1.4521, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3159945011138916 tensor(-107.4858)\n",
      "epoch 9 loss tensor(1.7300, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31699228286743164 tensor(-107.4990)\n",
      "epoch 10 loss tensor(0.5981, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3188967704772949 tensor(-107.4932)\n",
      "epoch 11 loss tensor(0.3316, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32277369499206543 tensor(-107.4957)\n",
      "epoch 12 loss tensor(1.3579, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3192150592803955 tensor(-107.4987)\n",
      "epoch 13 loss tensor(0.2142, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31993770599365234 tensor(-107.4860)\n",
      "epoch 14 loss tensor(6.9267, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31802845001220703 tensor(-107.4866)\n",
      "epoch 15 loss tensor(6.6271, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31723952293395996 tensor(-107.4814)\n",
      "epoch 16 loss tensor(1.0851, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31513237953186035 tensor(-107.4854)\n",
      "epoch 17 loss tensor(0.0405, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3127164840698242 tensor(-107.4951)\n",
      "epoch 18 loss tensor(0.2571, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3170044422149658 tensor(-107.5049)\n",
      "epoch 19 loss tensor(22.2919, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3184373378753662 tensor(-107.4996)\n",
      "epoch 20 loss tensor(20.1999, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32134270668029785 tensor(-107.4233)\n",
      "epoch 21 loss tensor(11.6528, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31812357902526855 tensor(-107.4878)\n",
      "epoch 22 loss tensor(2.7671, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31828808784484863 tensor(-107.5043)\n",
      "epoch 23 loss tensor(0.0625, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3305177688598633 tensor(-107.5067)\n",
      "epoch 24 loss tensor(8.9649, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.34533166885375977 tensor(-107.4754)\n",
      "epoch 25 loss tensor(16.4175, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3177835941314697 tensor(-107.1862)\n",
      "epoch 26 loss tensor(0.2289, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32276082038879395 tensor(-107.4535)\n",
      "epoch 27 loss tensor(0.2279, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32374048233032227 tensor(-107.4891)\n",
      "epoch 28 loss tensor(3.5908, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3249480724334717 tensor(-107.4820)\n",
      "epoch 29 loss tensor(0.8495, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31037402153015137 tensor(-107.4910)\n",
      "epoch 30 loss tensor(0.0179, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31162548065185547 tensor(-107.4755)\n",
      "epoch 31 loss tensor(0.6213, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30783677101135254 tensor(-107.4945)\n",
      "epoch 32 loss tensor(5.2406, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3071873188018799 tensor(-107.4914)\n",
      "epoch 33 loss tensor(2.6352, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3122124671936035 tensor(-107.4969)\n",
      "epoch 34 loss tensor(1.0674, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30396080017089844 tensor(-107.4596)\n",
      "epoch 35 loss tensor(0.4032, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31064581871032715 tensor(-107.4922)\n",
      "epoch 36 loss tensor(10.3608, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30388760566711426 tensor(-107.3471)\n",
      "epoch 37 loss tensor(1.9469, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3151426315307617 tensor(-107.4812)\n",
      "epoch 38 loss tensor(0.5243, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31914687156677246 tensor(-107.4528)\n",
      "epoch 39 loss tensor(1.0506, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.317410945892334 tensor(-107.4903)\n",
      "epoch 40 loss tensor(3.9954, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3155665397644043 tensor(-107.4589)\n",
      "epoch 41 loss tensor(1.3394, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31187987327575684 tensor(-107.4713)\n",
      "epoch 42 loss tensor(0.1523, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31285953521728516 tensor(-107.4865)\n",
      "epoch 43 loss tensor(0.5268, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3197135925292969 tensor(-107.4950)\n",
      "epoch 44 loss tensor(8.4593, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31984424591064453 tensor(-107.4851)\n",
      "epoch 45 loss tensor(6.2793, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3193023204803467 tensor(-107.4969)\n",
      "epoch 46 loss tensor(2.7685, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31888294219970703 tensor(-107.4943)\n",
      "epoch 47 loss tensor(0.0584, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3116133213043213 tensor(-107.4915)\n",
      "epoch 48 loss tensor(8.5339, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.312650203704834 tensor(-107.4744)\n",
      "epoch 49 loss tensor(7.6415, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30570459365844727 tensor(-107.4862)\n",
      "epoch 50 loss tensor(1.2312, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3090023994445801 tensor(-107.4890)\n",
      "epoch 51 loss tensor(0.2324, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30953383445739746 tensor(-107.4968)\n",
      "epoch 52 loss tensor(14.0039, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31282734870910645 tensor(-107.4909)\n",
      "epoch 53 loss tensor(11.8446, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31244921684265137 tensor(-107.4932)\n",
      "epoch 54 loss tensor(2.8389, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3067741394042969 tensor(-107.4838)\n",
      "epoch 55 loss tensor(1.2945, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3098599910736084 tensor(-107.3339)\n",
      "epoch 56 loss tensor(3.9826, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3135833740234375 tensor(-107.4878)\n",
      "epoch 57 loss tensor(0.3143, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32102537155151367 tensor(-107.4863)\n",
      "epoch 58 loss tensor(17.6144, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3326222896575928 tensor(-107.4858)\n",
      "epoch 59 loss tensor(35.0305, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32534193992614746 tensor(-107.4836)\n",
      "epoch 60 loss tensor(65.7919, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3199131488800049 tensor(-107.4651)\n",
      "epoch 61 loss tensor(44.1211, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31459760665893555 tensor(-107.4742)\n",
      "epoch 62 loss tensor(43.7972, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31348323822021484 tensor(-107.4374)\n",
      "epoch 63 loss tensor(9.4386, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3177006244659424 tensor(-107.4704)\n",
      "epoch 64 loss tensor(0.4072, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31718015670776367 tensor(-107.4762)\n",
      "epoch 65 loss tensor(1.3969, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31815099716186523 tensor(-107.4915)\n",
      "epoch 66 loss tensor(0.0970, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31533265113830566 tensor(-107.5014)\n",
      "epoch 67 loss tensor(21.2401, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3158419132232666 tensor(-107.4823)\n",
      "epoch 68 loss tensor(23.9161, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31843137741088867 tensor(-107.4664)\n",
      "epoch 69 loss tensor(10.5567, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32268428802490234 tensor(-107.4839)\n",
      "epoch 70 loss tensor(8.6688, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32267022132873535 tensor(-107.4422)\n",
      "epoch 71 loss tensor(1.2018, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32323527336120605 tensor(-107.4928)\n",
      "epoch 72 loss tensor(0.5692, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3235154151916504 tensor(-107.4845)\n",
      "epoch 73 loss tensor(3.7879, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32040977478027344 tensor(-107.5087)\n",
      "epoch 74 loss tensor(3.1666, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31774473190307617 tensor(-107.4445)\n",
      "epoch 75 loss tensor(0.7777, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3212251663208008 tensor(-107.4274)\n",
      "epoch 76 loss tensor(2.7968, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32012486457824707 tensor(-107.4741)\n",
      "epoch 77 loss tensor(0.4957, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3141331672668457 tensor(-107.4734)\n",
      "epoch 78 loss tensor(14.4026, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3182716369628906 tensor(-107.4527)\n",
      "epoch 79 loss tensor(34.8129, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31757068634033203 tensor(-107.4900)\n",
      "epoch 80 loss tensor(97.7051, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32082676887512207 tensor(-107.4632)\n",
      "epoch 81 loss tensor(45.2721, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3179495334625244 tensor(-107.4633)\n",
      "epoch 82 loss tensor(23.6884, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3168604373931885 tensor(-107.4585)\n",
      "epoch 83 loss tensor(2.5909, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.315493106842041 tensor(-107.4851)\n",
      "epoch 84 loss tensor(2.8787, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.316115140914917 tensor(-107.0750)\n",
      "epoch 85 loss tensor(3.2163, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32024407386779785 tensor(-107.4443)\n",
      "epoch 86 loss tensor(1.0855, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3212594985961914 tensor(-107.3620)\n",
      "epoch 87 loss tensor(0.1857, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31740856170654297 tensor(-107.4510)\n",
      "epoch 88 loss tensor(2.0574, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3274078369140625 tensor(-107.3376)\n",
      "epoch 89 loss tensor(0.6265, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32013702392578125 tensor(-107.4908)\n",
      "epoch 90 loss tensor(5.2419, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32240915298461914 tensor(-107.4937)\n",
      "epoch 91 loss tensor(9.4816, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32872843742370605 tensor(-107.4893)\n",
      "epoch 92 loss tensor(1.1518, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3257272243499756 tensor(-107.5004)\n",
      "epoch 93 loss tensor(0.1005, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3206486701965332 tensor(-107.4870)\n",
      "epoch 94 loss tensor(1.8453, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3200359344482422 tensor(-107.4718)\n",
      "epoch 95 loss tensor(1.2437, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3193662166595459 tensor(-107.4703)\n",
      "epoch 96 loss tensor(0.6168, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.318204402923584 tensor(-107.4735)\n",
      "epoch 97 loss tensor(2.3606, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32003140449523926 tensor(-107.4001)\n",
      "epoch 98 loss tensor(0.2040, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3235592842102051 tensor(-107.4929)\n",
      "epoch 99 loss tensor(9.3947, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32180118560791016 tensor(-107.4924)\n",
      "Log saved at: experiments_logs/gqe_sweep/lr_0.001/metrics.csv\n",
      "\n",
      "Running with lr = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 86.92M\n",
      "epoch 0 loss tensor(0.4556, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33182358741760254 tensor(-107.4851)\n",
      "epoch 1 loss tensor(589.6943, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32665252685546875 tensor(-107.4254)\n",
      "epoch 2 loss tensor(1.9331, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3209414482116699 tensor(-107.4511)\n",
      "epoch 3 loss tensor(1.7309, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3202052116394043 tensor(-107.4873)\n",
      "epoch 4 loss tensor(28199.1348, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3233160972595215 tensor(-107.3943)\n",
      "epoch 5 loss tensor(10481.5342, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3141615390777588 tensor(-107.3077)\n",
      "epoch 6 loss tensor(124.3686, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31334447860717773 tensor(-107.4735)\n",
      "epoch 7 loss tensor(175.8037, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31835269927978516 tensor(-107.4235)\n",
      "epoch 8 loss tensor(0.7377, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31314897537231445 tensor(-107.4839)\n",
      "epoch 9 loss tensor(0.0907, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31580066680908203 tensor(-107.4813)\n",
      "epoch 10 loss tensor(1.3249, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3151888847351074 tensor(-107.4854)\n",
      "epoch 11 loss tensor(8.9695, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3177683353424072 tensor(-107.4884)\n",
      "epoch 12 loss tensor(1.7324, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31456780433654785 tensor(-107.4899)\n",
      "epoch 13 loss tensor(0.1450, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31618237495422363 tensor(-107.4918)\n",
      "epoch 14 loss tensor(0.7807, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3144245147705078 tensor(-107.4813)\n",
      "epoch 15 loss tensor(0.2758, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31407666206359863 tensor(-107.4854)\n",
      "epoch 16 loss tensor(0.0961, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31558966636657715 tensor(-107.4963)\n",
      "epoch 17 loss tensor(1.8676, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31836724281311035 tensor(-107.4935)\n",
      "epoch 18 loss tensor(1.1737, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32271838188171387 tensor(-107.4919)\n",
      "epoch 19 loss tensor(0.1042, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32265782356262207 tensor(-107.4894)\n",
      "epoch 20 loss tensor(0.3017, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3256247043609619 tensor(-107.4853)\n",
      "epoch 21 loss tensor(0.2174, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32756471633911133 tensor(-107.4741)\n",
      "epoch 22 loss tensor(0.3375, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32080984115600586 tensor(-107.4849)\n",
      "epoch 23 loss tensor(0.1094, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.323087215423584 tensor(-107.4850)\n",
      "epoch 24 loss tensor(8.5590, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31720590591430664 tensor(-107.4874)\n",
      "epoch 25 loss tensor(6.9664, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3203880786895752 tensor(-107.4828)\n",
      "epoch 26 loss tensor(4.7550, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3230626583099365 tensor(-107.4815)\n",
      "epoch 27 loss tensor(2.5810, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3258979320526123 tensor(-107.4917)\n",
      "epoch 28 loss tensor(1.3655, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32462310791015625 tensor(-107.4697)\n",
      "epoch 29 loss tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32245659828186035 tensor(-107.5029)\n",
      "epoch 30 loss tensor(0.1925, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32327699661254883 tensor(-107.5011)\n",
      "epoch 31 loss tensor(1.9584, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32720065116882324 tensor(-107.4927)\n",
      "epoch 32 loss tensor(1.8557, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32353758811950684 tensor(-107.5059)\n",
      "epoch 33 loss tensor(0.4055, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3278987407684326 tensor(-107.5196)\n",
      "epoch 34 loss tensor(0.0166, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3269805908203125 tensor(-107.5168)\n",
      "epoch 35 loss tensor(0.1914, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3271481990814209 tensor(-107.5189)\n",
      "epoch 36 loss tensor(0.0255, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3305783271789551 tensor(-107.5017)\n",
      "epoch 37 loss tensor(0.3787, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32578039169311523 tensor(-107.5066)\n",
      "epoch 38 loss tensor(0.1404, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32122063636779785 tensor(-107.5206)\n",
      "epoch 39 loss tensor(0.0799, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32269835472106934 tensor(-107.4911)\n",
      "epoch 40 loss tensor(1.1048, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3194293975830078 tensor(-107.4677)\n",
      "epoch 41 loss tensor(0.7845, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3213512897491455 tensor(-107.4650)\n",
      "epoch 42 loss tensor(0.0431, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3411233425140381 tensor(-107.4783)\n",
      "epoch 43 loss tensor(0.3857, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33843445777893066 tensor(-107.4887)\n",
      "epoch 44 loss tensor(0.0816, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31989026069641113 tensor(-107.4887)\n",
      "epoch 45 loss tensor(0.0928, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31827521324157715 tensor(-107.4751)\n",
      "epoch 46 loss tensor(0.0424, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3237297534942627 tensor(-107.4823)\n",
      "epoch 47 loss tensor(1.9743, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3185451030731201 tensor(-107.4476)\n",
      "epoch 48 loss tensor(1.8572, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31967806816101074 tensor(-107.4817)\n",
      "epoch 49 loss tensor(0.4621, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3193833827972412 tensor(-107.4847)\n",
      "epoch 50 loss tensor(0.0695, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3220996856689453 tensor(-107.4564)\n",
      "epoch 51 loss tensor(0.1417, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32367849349975586 tensor(-107.4773)\n",
      "epoch 52 loss tensor(0.0769, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32273125648498535 tensor(-107.4899)\n",
      "epoch 53 loss tensor(0.1160, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3143918514251709 tensor(-107.4770)\n",
      "epoch 54 loss tensor(0.1025, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31395912170410156 tensor(-107.4757)\n",
      "epoch 55 loss tensor(0.0220, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3123481273651123 tensor(-107.4912)\n",
      "epoch 56 loss tensor(0.0946, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3116593360900879 tensor(-107.4914)\n",
      "epoch 57 loss tensor(2.5253, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3104832172393799 tensor(-107.4934)\n",
      "epoch 58 loss tensor(1.9754, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.30705952644348145 tensor(-107.4934)\n",
      "epoch 59 loss tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31812071800231934 tensor(-107.4895)\n",
      "epoch 60 loss tensor(0.3313, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31375837326049805 tensor(-107.4923)\n",
      "epoch 61 loss tensor(0.0524, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3151509761810303 tensor(-107.4864)\n",
      "epoch 62 loss tensor(2.9037, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31804394721984863 tensor(-107.4852)\n",
      "epoch 63 loss tensor(2.8747, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3188960552215576 tensor(-107.4906)\n",
      "epoch 64 loss tensor(1.4815, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.316558837890625 tensor(-107.4744)\n",
      "epoch 65 loss tensor(0.4748, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32152414321899414 tensor(-107.4781)\n",
      "epoch 66 loss tensor(0.1187, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.317690372467041 tensor(-107.4662)\n",
      "epoch 67 loss tensor(0.0942, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3218860626220703 tensor(-107.4878)\n",
      "epoch 68 loss tensor(0.1379, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3445887565612793 tensor(-107.4801)\n",
      "epoch 69 loss tensor(0.0857, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32639265060424805 tensor(-107.4842)\n",
      "epoch 70 loss tensor(0.1118, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.312488317489624 tensor(-107.4914)\n",
      "epoch 71 loss tensor(0.3868, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31418371200561523 tensor(-107.4932)\n",
      "epoch 72 loss tensor(0.1896, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32196831703186035 tensor(-107.4931)\n",
      "epoch 73 loss tensor(0.2454, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31916260719299316 tensor(-107.4938)\n",
      "epoch 74 loss tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31885695457458496 tensor(-107.4977)\n",
      "epoch 75 loss tensor(3.9027, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3337526321411133 tensor(-107.4971)\n",
      "epoch 76 loss tensor(4.1301, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.34946680068969727 tensor(-107.4972)\n",
      "epoch 77 loss tensor(1.5399, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3169877529144287 tensor(-107.4933)\n",
      "epoch 78 loss tensor(0.3795, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3202826976776123 tensor(-107.4897)\n",
      "epoch 79 loss tensor(0.0161, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3206355571746826 tensor(-107.4763)\n",
      "epoch 80 loss tensor(0.2652, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3249533176422119 tensor(-107.4824)\n",
      "epoch 81 loss tensor(0.0679, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3206963539123535 tensor(-107.4902)\n",
      "epoch 82 loss tensor(2.1297, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3214995861053467 tensor(-107.5086)\n",
      "epoch 83 loss tensor(2.3914, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32256531715393066 tensor(-107.5073)\n",
      "epoch 84 loss tensor(1.1849, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3209047317504883 tensor(-107.4926)\n",
      "epoch 85 loss tensor(0.5057, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3236887454986572 tensor(-107.4820)\n",
      "epoch 86 loss tensor(0.0310, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3195962905883789 tensor(-107.4792)\n",
      "epoch 87 loss tensor(0.0586, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32108592987060547 tensor(-107.4786)\n",
      "epoch 88 loss tensor(0.0233, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32250475883483887 tensor(-107.4850)\n",
      "epoch 89 loss tensor(0.1040, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32363128662109375 tensor(-107.4906)\n",
      "epoch 90 loss tensor(1.8218, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32529258728027344 tensor(-107.4660)\n",
      "epoch 91 loss tensor(1.0173, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3257622718811035 tensor(-107.4760)\n",
      "epoch 92 loss tensor(0.0627, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31992506980895996 tensor(-107.4828)\n",
      "epoch 93 loss tensor(0.1021, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3199338912963867 tensor(-107.4974)\n",
      "epoch 94 loss tensor(0.1908, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3190345764160156 tensor(-107.4946)\n",
      "epoch 95 loss tensor(0.2167, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3202643394470215 tensor(-107.4926)\n",
      "epoch 96 loss tensor(0.0385, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32047009468078613 tensor(-107.4936)\n",
      "epoch 97 loss tensor(0.0408, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32212305068969727 tensor(-107.4883)\n",
      "epoch 98 loss tensor(0.0666, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3276042938232422 tensor(-107.4933)\n",
      "epoch 99 loss tensor(0.5466, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32651257514953613 tensor(-107.4945)\n",
      "Log saved at: experiments_logs/gqe_sweep/lr_0.0001/metrics.csv\n",
      "\n",
      "Running with lr = 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 86.92M\n",
      "epoch 0 loss tensor(0.4556, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33405113220214844 tensor(-107.4851)\n",
      "epoch 1 loss tensor(62.6909, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32781505584716797 tensor(-107.4583)\n",
      "epoch 2 loss tensor(5.7108, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32176828384399414 tensor(-107.4910)\n",
      "epoch 3 loss tensor(0.0749, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31924867630004883 tensor(-107.4922)\n",
      "epoch 4 loss tensor(0.2938, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3215155601501465 tensor(-107.5043)\n",
      "epoch 5 loss tensor(0.6736, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3218650817871094 tensor(-107.4924)\n",
      "epoch 6 loss tensor(0.1053, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3268709182739258 tensor(-107.4922)\n",
      "epoch 7 loss tensor(2.1157, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32294631004333496 tensor(-107.4930)\n",
      "epoch 8 loss tensor(0.9757, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31905055046081543 tensor(-107.4934)\n",
      "epoch 9 loss tensor(0.4604, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31760406494140625 tensor(-107.4888)\n",
      "epoch 10 loss tensor(0.1545, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31746339797973633 tensor(-107.4907)\n",
      "epoch 11 loss tensor(0.3161, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31366753578186035 tensor(-107.4830)\n",
      "epoch 12 loss tensor(2.6443, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3176908493041992 tensor(-107.4182)\n",
      "epoch 13 loss tensor(0.5862, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3159599304199219 tensor(-107.4487)\n",
      "epoch 14 loss tensor(0.1816, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32189321517944336 tensor(-107.4863)\n",
      "epoch 15 loss tensor(0.2737, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31568479537963867 tensor(-107.4842)\n",
      "epoch 16 loss tensor(0.0537, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3166966438293457 tensor(-107.4959)\n",
      "epoch 17 loss tensor(0.5290, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3198552131652832 tensor(-107.4817)\n",
      "epoch 18 loss tensor(0.1779, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3256089687347412 tensor(-107.4979)\n",
      "epoch 19 loss tensor(0.0705, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3235816955566406 tensor(-107.5008)\n",
      "epoch 20 loss tensor(0.1410, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3236386775970459 tensor(-107.4839)\n",
      "epoch 21 loss tensor(0.3687, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3250131607055664 tensor(-107.4965)\n",
      "epoch 22 loss tensor(0.1894, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3247640132904053 tensor(-107.4886)\n",
      "epoch 23 loss tensor(0.3590, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3237588405609131 tensor(-107.4985)\n",
      "epoch 24 loss tensor(0.1746, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32181525230407715 tensor(-107.4878)\n",
      "epoch 25 loss tensor(1.2841, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32266998291015625 tensor(-107.5073)\n",
      "epoch 26 loss tensor(0.8122, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32248425483703613 tensor(-107.4991)\n",
      "epoch 27 loss tensor(0.1186, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.324573278427124 tensor(-107.4972)\n",
      "epoch 28 loss tensor(0.0896, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3267059326171875 tensor(-107.4811)\n",
      "epoch 29 loss tensor(0.1575, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32298707962036133 tensor(-107.4979)\n",
      "epoch 30 loss tensor(0.1775, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32270336151123047 tensor(-107.4865)\n",
      "epoch 31 loss tensor(0.0988, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32095980644226074 tensor(-107.4775)\n",
      "epoch 32 loss tensor(0.1136, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3220813274383545 tensor(-107.4574)\n",
      "epoch 33 loss tensor(2.0029, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32274770736694336 tensor(-107.4356)\n",
      "epoch 34 loss tensor(2.1403, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3211650848388672 tensor(-107.4091)\n",
      "epoch 35 loss tensor(0.7042, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32199788093566895 tensor(-107.4500)\n",
      "epoch 36 loss tensor(0.1327, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32158756256103516 tensor(-107.4626)\n",
      "epoch 37 loss tensor(0.3619, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32088184356689453 tensor(-107.4536)\n",
      "epoch 38 loss tensor(0.1737, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3261697292327881 tensor(-107.4598)\n",
      "epoch 39 loss tensor(0.2774, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3235034942626953 tensor(-107.4839)\n",
      "epoch 40 loss tensor(0.1444, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32435131072998047 tensor(-107.4816)\n",
      "epoch 41 loss tensor(2.2457, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3217165470123291 tensor(-107.4273)\n",
      "epoch 42 loss tensor(2.4652, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31723761558532715 tensor(-107.4818)\n",
      "epoch 43 loss tensor(0.6686, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3256978988647461 tensor(-107.4939)\n",
      "epoch 44 loss tensor(0.0454, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3210122585296631 tensor(-107.4870)\n",
      "epoch 45 loss tensor(0.1377, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3193192481994629 tensor(-107.4894)\n",
      "epoch 46 loss tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3220813274383545 tensor(-107.4968)\n",
      "epoch 47 loss tensor(0.1641, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3200058937072754 tensor(-107.5080)\n",
      "epoch 48 loss tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3220360279083252 tensor(-107.4979)\n",
      "epoch 49 loss tensor(0.3474, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3163280487060547 tensor(-107.4920)\n",
      "epoch 50 loss tensor(0.1367, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3245227336883545 tensor(-107.4938)\n",
      "epoch 51 loss tensor(0.1136, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3275144100189209 tensor(-107.4906)\n",
      "epoch 52 loss tensor(0.0756, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3253936767578125 tensor(-107.4793)\n",
      "epoch 53 loss tensor(1.6498, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3240509033203125 tensor(-107.4573)\n",
      "epoch 54 loss tensor(2.1363, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32448506355285645 tensor(-107.4589)\n",
      "epoch 55 loss tensor(0.9636, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3191640377044678 tensor(-107.4783)\n",
      "epoch 56 loss tensor(0.0760, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3185427188873291 tensor(-107.4915)\n",
      "epoch 57 loss tensor(0.0786, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3189208507537842 tensor(-107.4840)\n",
      "epoch 58 loss tensor(0.0558, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32056403160095215 tensor(-107.4867)\n",
      "epoch 59 loss tensor(0.0161, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32761645317077637 tensor(-107.4832)\n",
      "epoch 60 loss tensor(0.0797, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32495737075805664 tensor(-107.4594)\n",
      "epoch 61 loss tensor(0.0763, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32216429710388184 tensor(-107.4676)\n",
      "epoch 62 loss tensor(0.1418, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32227253913879395 tensor(-107.4820)\n",
      "epoch 63 loss tensor(0.0781, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32031750679016113 tensor(-107.4938)\n",
      "epoch 64 loss tensor(0.1850, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32034993171691895 tensor(-107.4827)\n",
      "epoch 65 loss tensor(0.2521, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3224523067474365 tensor(-107.4831)\n",
      "epoch 66 loss tensor(0.0299, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32319092750549316 tensor(-107.4966)\n",
      "epoch 67 loss tensor(0.0741, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3181188106536865 tensor(-107.4920)\n",
      "epoch 68 loss tensor(0.0391, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3181626796722412 tensor(-107.4882)\n",
      "epoch 69 loss tensor(0.0984, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31756091117858887 tensor(-107.4736)\n",
      "epoch 70 loss tensor(0.1962, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3180973529815674 tensor(-107.4911)\n",
      "epoch 71 loss tensor(0.0417, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31995534896850586 tensor(-107.4873)\n",
      "epoch 72 loss tensor(0.4485, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32608747482299805 tensor(-107.4821)\n",
      "epoch 73 loss tensor(0.7901, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3218860626220703 tensor(-107.4899)\n",
      "epoch 74 loss tensor(0.3149, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32392454147338867 tensor(-107.4813)\n",
      "epoch 75 loss tensor(0.0255, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32817554473876953 tensor(-107.4881)\n",
      "epoch 76 loss tensor(0.1342, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3238229751586914 tensor(-107.4895)\n",
      "epoch 77 loss tensor(0.0205, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3262181282043457 tensor(-107.4915)\n",
      "epoch 78 loss tensor(0.8215, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31682467460632324 tensor(-107.4944)\n",
      "epoch 79 loss tensor(1.0744, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3160836696624756 tensor(-107.4942)\n",
      "epoch 80 loss tensor(0.3966, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3153061866760254 tensor(-107.4890)\n",
      "epoch 81 loss tensor(0.0371, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32032108306884766 tensor(-107.4851)\n",
      "epoch 82 loss tensor(0.2049, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31740236282348633 tensor(-107.4871)\n",
      "epoch 83 loss tensor(0.0442, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3192901611328125 tensor(-107.4963)\n",
      "epoch 84 loss tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3191797733306885 tensor(-107.4965)\n",
      "epoch 85 loss tensor(0.2054, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32112932205200195 tensor(-107.5076)\n",
      "epoch 86 loss tensor(0.4424, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32062816619873047 tensor(-107.4950)\n",
      "epoch 87 loss tensor(0.1287, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3220810890197754 tensor(-107.5043)\n",
      "epoch 88 loss tensor(0.0876, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32295846939086914 tensor(-107.5029)\n",
      "epoch 89 loss tensor(0.0568, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31778383255004883 tensor(-107.5023)\n",
      "epoch 90 loss tensor(0.3874, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32064104080200195 tensor(-107.4987)\n",
      "epoch 91 loss tensor(0.9207, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3237483501434326 tensor(-107.5005)\n",
      "epoch 92 loss tensor(0.0522, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32256412506103516 tensor(-107.4973)\n",
      "epoch 93 loss tensor(0.0656, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3191092014312744 tensor(-107.4934)\n",
      "epoch 94 loss tensor(0.0215, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31923556327819824 tensor(-107.4967)\n",
      "epoch 95 loss tensor(0.0381, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3166649341583252 tensor(-107.4943)\n",
      "epoch 96 loss tensor(0.0945, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3187127113342285 tensor(-107.4955)\n",
      "epoch 97 loss tensor(0.0159, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3236701488494873 tensor(-107.4979)\n",
      "epoch 98 loss tensor(0.2975, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3191499710083008 tensor(-107.4829)\n",
      "epoch 99 loss tensor(0.0228, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3164021968841553 tensor(-107.4782)\n",
      "Log saved at: experiments_logs/gqe_sweep/lr_1e-05/metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2519516/3677365917.py:75: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from lightning.fabric.loggers import CSVLogger\n",
    "\n",
    "# Á¢∫‰øù GQE Áõ∏ÈóúË®≠ÂÆöÂ∑≤Á∂ìËºâÂÖ• (cfg, op_pool, cost, gqe Á≠â)\n",
    "\n",
    "def run_parameter_sweep(param_name, param_values):\n",
    "    \"\"\"\n",
    "    Âü∑Ë°åÂèÉÊï∏ÊéÉÊèèÂØ¶È©ó (‰øÆÊ≠£ÁâàÔºöÂº∑Âà∂ÈñãÂïü Logging)\n",
    "    \"\"\"\n",
    "    log_paths = {}\n",
    "    \n",
    "    print(f\"=== Starting Sweep for {param_name} ===\")\n",
    "    \n",
    "    # üíé ÈóúÈçµ‰øÆÊ≠£ÔºöÁ¢∫‰øù GQE ÊúÉÂØ´ÂÖ• Log\n",
    "    cfg.use_fabric_logging = True \n",
    "    \n",
    "    for val in param_values:\n",
    "        print(f\"\\nRunning with {param_name} = {val}\")\n",
    "        \n",
    "        # 1. ‰øÆÊîπ Config\n",
    "        setattr(cfg, param_name, val)\n",
    "        \n",
    "        # 2. Ë®≠ÂÆöÁç®Á´ãÁöÑ Logger\n",
    "        # ÁõÆÈåÑÁµêÊßã: experiments_logs/gqe_sweep/{version_name}/metrics.csv\n",
    "        version_name = f\"{param_name}_{val}\"\n",
    "        save_dir = \"experiments_logs\"\n",
    "        exp_name = \"gqe_sweep\"\n",
    "        \n",
    "        # Âª∫Á´ã Logger\n",
    "        logger = CSVLogger(save_dir, name=exp_name, version=version_name)\n",
    "        cfg.fabric_logger = logger\n",
    "        \n",
    "        # 3. Âü∑Ë°å GQE\n",
    "        # ÊàëÂÄëÂè™Ë∑ë 50-100 iters Â∞±Ë∂≥Â§†ËßÄÂØüË∂®Âã¢‰∫Ü\n",
    "        gqe(cost, op_pool, max_iters=100, ngates=20, config=cfg)\n",
    "        \n",
    "        # 4. Â∞ãÊâæ‰∏¶Ë®òÈåÑ Log Ê™îÊ°à‰ΩçÁΩÆ\n",
    "        # ÊúâÊôÇÂÄô Lightning ÊúÉÂ§ö‰∏ÄÂ±§ version_0 ÁõÆÈåÑÔºåÊàëÂÄëÁî® glob Ëá™ÂãïÂ∞ãÊâæÊØîËºÉ‰øùÈö™\n",
    "        search_pattern = os.path.join(save_dir, exp_name, version_name, \"**/metrics.csv\")\n",
    "        found_files = glob.glob(search_pattern, recursive=True)\n",
    "        \n",
    "        if found_files:\n",
    "            log_paths[val] = found_files[0] # ÂèñÁ¨¨‰∏ÄÂÄãÊâæÂà∞ÁöÑ\n",
    "            print(f\"Log saved at: {found_files[0]}\")\n",
    "        else:\n",
    "            # ÂÇôÁî®Ë∑ØÂæëÁåúÊ∏¨\n",
    "            fallback_path = os.path.join(save_dir, exp_name, version_name, \"metrics.csv\")\n",
    "            log_paths[val] = fallback_path\n",
    "            print(f\"Warning: File not immediately found via glob. Assume path: {fallback_path}\")\n",
    "        \n",
    "    return log_paths\n",
    "\n",
    "def plot_loss_curves(log_paths, param_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for val, path in log_paths.items():\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                # Áπ™Ë£Ω loss (ÈÅéÊøæÊéâ NaN)\n",
    "                if 'loss' in df.columns:\n",
    "                    data = df['loss'].dropna().reset_index(drop=True)\n",
    "                    plt.plot(data, label=f\"{param_name}={val}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {path}: {e}\")\n",
    "        else:\n",
    "            print(f\"File still not found: {path}\")\n",
    "            \n",
    "    plt.title(f\"GQE Loss Curve Comparison ({param_name})\")\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log') # Â∞çÊï∏Â∫ßÊ®ôÁúãÂ∑ÆÁï∞Êõ¥Ê∏ÖÊ•ö\n",
    "    plt.show()\n",
    "\n",
    "# === Âü∑Ë°åÂØ¶È©ó ===\n",
    "# Ê∏¨Ë©¶‰∏âÁ®Æ Learning Rate\n",
    "lr_values = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "# Âü∑Ë°åÊéÉÊèè\n",
    "lr_logs = run_parameter_sweep('lr', lr_values)\n",
    "\n",
    "# Áï´Âúñ\n",
    "plot_loss_curves(lr_logs, 'Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7768954-71a5-48ec-80b9-c064ecfde73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "be5dad02-b2e1-4110-8c0f-5380c28e22a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Sweep for temperature ===\n",
      "\n",
      "Running with temperature = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 86.92M\n",
      "epoch 0 loss tensor(11.5091, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3201131820678711 tensor(-107.4996)\n",
      "epoch 1 loss tensor(10.1792, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3143284320831299 tensor(-107.4829)\n",
      "epoch 2 loss tensor(9.9120, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31846189498901367 tensor(-107.4928)\n",
      "epoch 3 loss tensor(9.5947, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3180544376373291 tensor(-107.4799)\n",
      "epoch 4 loss tensor(9.3684, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3213789463043213 tensor(-107.5036)\n",
      "epoch 5 loss tensor(8.6540, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31772375106811523 tensor(-107.4945)\n",
      "epoch 6 loss tensor(7.3296, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3198113441467285 tensor(-107.4853)\n",
      "epoch 7 loss tensor(7.2416, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3186044692993164 tensor(-107.4911)\n",
      "epoch 8 loss tensor(6.8250, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3154158592224121 tensor(-107.4972)\n",
      "epoch 9 loss tensor(6.8361, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31637096405029297 tensor(-107.4897)\n",
      "epoch 10 loss tensor(6.6415, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3122410774230957 tensor(-107.4940)\n",
      "epoch 11 loss tensor(6.8980, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31296825408935547 tensor(-107.4908)\n",
      "epoch 12 loss tensor(5.3094, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3159003257751465 tensor(-107.5048)\n",
      "epoch 13 loss tensor(5.6345, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31789398193359375 tensor(-107.4905)\n",
      "epoch 14 loss tensor(4.7907, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3121762275695801 tensor(-107.4859)\n",
      "epoch 15 loss tensor(4.9739, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31217360496520996 tensor(-107.4788)\n",
      "epoch 16 loss tensor(3.3025, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3101341724395752 tensor(-107.5135)\n",
      "epoch 17 loss tensor(2.8872, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3089005947113037 tensor(-107.5003)\n",
      "epoch 18 loss tensor(2.6811, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3097348213195801 tensor(-107.4917)\n",
      "epoch 19 loss tensor(0.6465, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3138701915740967 tensor(-107.4820)\n",
      "epoch 20 loss tensor(1.0829, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32498979568481445 tensor(-107.4801)\n",
      "epoch 21 loss tensor(0.6100, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31794261932373047 tensor(-107.4799)\n",
      "epoch 22 loss tensor(16.3501, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3154749870300293 tensor(-107.4911)\n",
      "epoch 23 loss tensor(3.2503, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.316270112991333 tensor(-107.4795)\n",
      "epoch 24 loss tensor(4.9715, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31475305557250977 tensor(-107.4780)\n",
      "epoch 25 loss tensor(11.9224, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3160686492919922 tensor(-107.4844)\n",
      "epoch 26 loss tensor(5.4564, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3159451484680176 tensor(-107.4761)\n",
      "epoch 27 loss tensor(0.6556, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31934237480163574 tensor(-107.4949)\n",
      "epoch 28 loss tensor(0.8085, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3167862892150879 tensor(-107.4881)\n",
      "epoch 29 loss tensor(1.1785, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31738924980163574 tensor(-107.4874)\n",
      "epoch 30 loss tensor(0.5678, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3141641616821289 tensor(-107.4949)\n",
      "epoch 31 loss tensor(1.8051, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3121223449707031 tensor(-107.4837)\n",
      "epoch 32 loss tensor(0.7976, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31788039207458496 tensor(-107.4872)\n",
      "epoch 33 loss tensor(0.7004, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32213783264160156 tensor(-107.4900)\n",
      "epoch 34 loss tensor(0.8862, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3222367763519287 tensor(-107.4883)\n",
      "epoch 35 loss tensor(0.5933, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3239259719848633 tensor(-107.4991)\n",
      "epoch 36 loss tensor(0.6000, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3486673831939697 tensor(-107.4944)\n",
      "epoch 37 loss tensor(0.7936, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.34407949447631836 tensor(-107.4958)\n",
      "epoch 38 loss tensor(0.8848, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3223762512207031 tensor(-107.4851)\n",
      "epoch 39 loss tensor(1.0706, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32135629653930664 tensor(-107.4985)\n",
      "epoch 40 loss tensor(0.3058, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32281064987182617 tensor(-107.4879)\n",
      "epoch 41 loss tensor(0.1834, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32106685638427734 tensor(-107.4869)\n",
      "epoch 42 loss tensor(0.0330, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31847453117370605 tensor(-107.4922)\n",
      "epoch 43 loss tensor(0.2779, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3205888271331787 tensor(-107.4914)\n",
      "epoch 44 loss tensor(0.2191, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31702685356140137 tensor(-107.4862)\n",
      "epoch 45 loss tensor(0.3586, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3176705837249756 tensor(-107.4886)\n",
      "epoch 46 loss tensor(0.5933, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32150864601135254 tensor(-107.4921)\n",
      "epoch 47 loss tensor(0.3947, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3177063465118408 tensor(-107.4929)\n",
      "epoch 48 loss tensor(0.5130, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3224761486053467 tensor(-107.4867)\n",
      "epoch 49 loss tensor(0.5749, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32055115699768066 tensor(-107.4943)\n",
      "epoch 50 loss tensor(0.5596, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32222843170166016 tensor(-107.4907)\n",
      "epoch 51 loss tensor(0.2904, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32344913482666016 tensor(-107.4947)\n",
      "epoch 52 loss tensor(0.3488, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3266477584838867 tensor(-107.5011)\n",
      "epoch 53 loss tensor(0.0945, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31702303886413574 tensor(-107.4957)\n",
      "epoch 54 loss tensor(0.8366, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31961560249328613 tensor(-107.5040)\n",
      "epoch 55 loss tensor(0.4115, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3200857639312744 tensor(-107.4858)\n",
      "epoch 56 loss tensor(0.1351, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3192174434661865 tensor(-107.5013)\n",
      "epoch 57 loss tensor(0.1157, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3231635093688965 tensor(-107.4879)\n",
      "epoch 58 loss tensor(0.2489, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32027173042297363 tensor(-107.4938)\n",
      "epoch 59 loss tensor(0.8524, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32625675201416016 tensor(-107.4977)\n",
      "epoch 60 loss tensor(0.3072, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3212313652038574 tensor(-107.5045)\n",
      "epoch 61 loss tensor(0.2097, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31299829483032227 tensor(-107.4889)\n",
      "epoch 62 loss tensor(0.0848, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31308650970458984 tensor(-107.4967)\n",
      "epoch 63 loss tensor(0.2224, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31832051277160645 tensor(-107.4901)\n",
      "epoch 64 loss tensor(0.5807, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31503748893737793 tensor(-107.4959)\n",
      "epoch 65 loss tensor(0.5769, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31874966621398926 tensor(-107.4931)\n",
      "epoch 66 loss tensor(0.8077, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3146970272064209 tensor(-107.4917)\n",
      "epoch 67 loss tensor(0.5933, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3206768035888672 tensor(-107.4928)\n",
      "epoch 68 loss tensor(0.3451, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32082104682922363 tensor(-107.4995)\n",
      "epoch 69 loss tensor(0.5033, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31305837631225586 tensor(-107.4945)\n",
      "epoch 70 loss tensor(0.6384, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3136904239654541 tensor(-107.4969)\n",
      "epoch 71 loss tensor(0.3943, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3159327507019043 tensor(-107.4917)\n",
      "epoch 72 loss tensor(0.1388, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31912922859191895 tensor(-107.4996)\n",
      "epoch 73 loss tensor(0.1315, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3163313865661621 tensor(-107.4970)\n",
      "epoch 74 loss tensor(0.1156, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3143937587738037 tensor(-107.4927)\n",
      "epoch 75 loss tensor(0.2310, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3154587745666504 tensor(-107.4968)\n",
      "epoch 76 loss tensor(0.2572, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.313812255859375 tensor(-107.4907)\n",
      "epoch 77 loss tensor(0.4673, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3124828338623047 tensor(-107.4998)\n",
      "epoch 78 loss tensor(0.0527, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31067800521850586 tensor(-107.4995)\n",
      "epoch 79 loss tensor(0.0795, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3145480155944824 tensor(-107.4936)\n",
      "epoch 80 loss tensor(0.1052, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3131694793701172 tensor(-107.4914)\n",
      "epoch 81 loss tensor(0.2520, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3349292278289795 tensor(-107.4961)\n",
      "epoch 82 loss tensor(0.1852, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3269941806793213 tensor(-107.4931)\n",
      "epoch 83 loss tensor(0.1069, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31417083740234375 tensor(-107.4906)\n",
      "epoch 84 loss tensor(0.0613, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31830906867980957 tensor(-107.4969)\n",
      "epoch 85 loss tensor(0.3460, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31858325004577637 tensor(-107.4932)\n",
      "epoch 86 loss tensor(0.3778, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31827783584594727 tensor(-107.5044)\n",
      "epoch 87 loss tensor(0.0203, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3188292980194092 tensor(-107.4977)\n",
      "epoch 88 loss tensor(0.1223, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31725144386291504 tensor(-107.4911)\n",
      "epoch 89 loss tensor(0.0481, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31680798530578613 tensor(-107.4940)\n",
      "epoch 90 loss tensor(0.1067, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3179359436035156 tensor(-107.4964)\n",
      "epoch 91 loss tensor(0.1181, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3203246593475342 tensor(-107.4921)\n",
      "epoch 92 loss tensor(0.0304, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3238801956176758 tensor(-107.4982)\n",
      "epoch 93 loss tensor(0.3190, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32085204124450684 tensor(-107.4912)\n",
      "epoch 94 loss tensor(0.0779, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3221771717071533 tensor(-107.5019)\n",
      "epoch 95 loss tensor(0.3362, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32468700408935547 tensor(-107.4905)\n",
      "epoch 96 loss tensor(0.1103, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33460521697998047 tensor(-107.4909)\n",
      "epoch 97 loss tensor(0.0914, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3334684371948242 tensor(-107.4999)\n",
      "epoch 98 loss tensor(0.0478, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32134413719177246 tensor(-107.4896)\n",
      "epoch 99 loss tensor(0.3173, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3234696388244629 tensor(-107.4976)\n",
      "Log saved at: experiments_logs/gqe_sweep/temperature_0.1/metrics.csv\n",
      "\n",
      "Running with temperature = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 86.92M\n",
      "epoch 0 loss tensor(9.6810, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3213522434234619 tensor(-107.4915)\n",
      "epoch 1 loss tensor(6.9811, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3310549259185791 tensor(-107.4845)\n",
      "epoch 2 loss tensor(5.2610, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32577967643737793 tensor(-107.4930)\n",
      "epoch 3 loss tensor(1807.3453, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3230628967285156 tensor(-107.4474)\n",
      "epoch 4 loss tensor(4.8147, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32140564918518066 tensor(-107.5048)\n",
      "epoch 5 loss tensor(3.3717, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32398438453674316 tensor(-107.4926)\n",
      "epoch 6 loss tensor(1.8577, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3233211040496826 tensor(-107.4956)\n",
      "epoch 7 loss tensor(0.8445, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.318234920501709 tensor(-107.4919)\n",
      "epoch 8 loss tensor(37.0576, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3185250759124756 tensor(-107.4924)\n",
      "epoch 9 loss tensor(7.1620, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31766748428344727 tensor(-107.4925)\n",
      "epoch 10 loss tensor(0.6181, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32109832763671875 tensor(-107.4926)\n",
      "epoch 11 loss tensor(1.4070, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3260529041290283 tensor(-107.4883)\n",
      "epoch 12 loss tensor(0.4325, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32860445976257324 tensor(-107.4951)\n",
      "epoch 13 loss tensor(0.1932, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32591700553894043 tensor(-107.4921)\n",
      "epoch 14 loss tensor(0.7216, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32683396339416504 tensor(-107.4915)\n",
      "epoch 15 loss tensor(0.1022, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3295769691467285 tensor(-107.4831)\n",
      "epoch 16 loss tensor(1.0596, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3265113830566406 tensor(-107.5088)\n",
      "epoch 17 loss tensor(0.3201, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3284430503845215 tensor(-107.4884)\n",
      "epoch 18 loss tensor(0.6382, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32776737213134766 tensor(-107.4924)\n",
      "epoch 19 loss tensor(1.5677, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3335087299346924 tensor(-107.4821)\n",
      "epoch 20 loss tensor(0.3777, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3293120861053467 tensor(-107.4919)\n",
      "epoch 21 loss tensor(0.8814, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32710862159729004 tensor(-107.4956)\n",
      "epoch 22 loss tensor(0.2833, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3287618160247803 tensor(-107.4897)\n",
      "epoch 23 loss tensor(0.2158, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32901930809020996 tensor(-107.4871)\n",
      "epoch 24 loss tensor(0.4339, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32595205307006836 tensor(-107.4878)\n",
      "epoch 25 loss tensor(2.0435, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32784271240234375 tensor(-107.4870)\n",
      "epoch 26 loss tensor(0.2641, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33025264739990234 tensor(-107.4865)\n",
      "epoch 27 loss tensor(0.9940, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3238868713378906 tensor(-107.4990)\n",
      "epoch 28 loss tensor(0.3556, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31975531578063965 tensor(-107.5051)\n",
      "epoch 29 loss tensor(0.2892, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32022857666015625 tensor(-107.4878)\n",
      "epoch 30 loss tensor(0.8967, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.31891655921936035 tensor(-107.4959)\n",
      "epoch 31 loss tensor(0.6050, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32048463821411133 tensor(-107.4925)\n",
      "epoch 32 loss tensor(0.2777, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.325361967086792 tensor(-107.4975)\n",
      "epoch 33 loss tensor(1.3220, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.322340726852417 tensor(-107.4933)\n",
      "epoch 34 loss tensor(0.3690, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3375866413116455 tensor(-107.4962)\n",
      "epoch 35 loss tensor(0.5494, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3432142734527588 tensor(-107.4943)\n",
      "epoch 36 loss tensor(0.2929, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.318676233291626 tensor(-107.4959)\n",
      "epoch 37 loss tensor(1.1294, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32077622413635254 tensor(-107.4905)\n",
      "epoch 38 loss tensor(0.1531, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32170915603637695 tensor(-107.5054)\n",
      "epoch 39 loss tensor(0.2127, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32323431968688965 tensor(-107.5028)\n",
      "epoch 40 loss tensor(0.3298, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32056570053100586 tensor(-107.4921)\n",
      "epoch 41 loss tensor(0.6684, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.324221134185791 tensor(-107.4917)\n",
      "epoch 42 loss tensor(0.1046, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32645654678344727 tensor(-107.4894)\n",
      "epoch 43 loss tensor(0.2364, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3329956531524658 tensor(-107.4981)\n",
      "epoch 44 loss tensor(0.1207, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3266441822052002 tensor(-107.4943)\n",
      "epoch 45 loss tensor(0.2483, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3292348384857178 tensor(-107.4983)\n",
      "epoch 46 loss tensor(0.1243, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32476377487182617 tensor(-107.5002)\n",
      "epoch 47 loss tensor(0.5754, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.324324369430542 tensor(-107.4963)\n",
      "epoch 48 loss tensor(0.3030, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3260221481323242 tensor(-107.5049)\n",
      "epoch 49 loss tensor(0.3772, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32761216163635254 tensor(-107.5107)\n",
      "epoch 50 loss tensor(0.2717, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3241703510284424 tensor(-107.4930)\n",
      "epoch 51 loss tensor(0.4250, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33187031745910645 tensor(-107.4940)\n",
      "epoch 52 loss tensor(0.2218, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32741427421569824 tensor(-107.4939)\n",
      "epoch 53 loss tensor(0.1020, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3294546604156494 tensor(-107.4873)\n",
      "epoch 54 loss tensor(0.1395, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3293936252593994 tensor(-107.4985)\n",
      "epoch 55 loss tensor(0.1326, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3296482563018799 tensor(-107.4884)\n",
      "epoch 56 loss tensor(0.0959, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3301689624786377 tensor(-107.4850)\n",
      "epoch 57 loss tensor(0.1173, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3330657482147217 tensor(-107.4801)\n",
      "epoch 58 loss tensor(0.1747, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3342399597167969 tensor(-107.4904)\n",
      "epoch 59 loss tensor(0.1266, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3310079574584961 tensor(-107.4912)\n",
      "epoch 60 loss tensor(0.4471, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3299734592437744 tensor(-107.4748)\n",
      "epoch 61 loss tensor(0.6993, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33070850372314453 tensor(-107.4765)\n",
      "epoch 62 loss tensor(0.1410, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32924699783325195 tensor(-107.4816)\n",
      "epoch 63 loss tensor(0.5392, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33127903938293457 tensor(-107.5029)\n",
      "epoch 64 loss tensor(0.1686, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3329331874847412 tensor(-107.4845)\n",
      "epoch 65 loss tensor(0.1382, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3311460018157959 tensor(-107.4886)\n",
      "epoch 66 loss tensor(0.1864, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3277912139892578 tensor(-107.4924)\n",
      "epoch 67 loss tensor(0.1234, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32889437675476074 tensor(-107.4899)\n",
      "epoch 68 loss tensor(0.1022, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32810163497924805 tensor(-107.4964)\n",
      "epoch 69 loss tensor(0.0939, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3297746181488037 tensor(-107.4904)\n",
      "epoch 70 loss tensor(0.5367, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3340301513671875 tensor(-107.4849)\n",
      "epoch 71 loss tensor(0.1900, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3277444839477539 tensor(-107.4934)\n",
      "epoch 72 loss tensor(0.0366, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3295435905456543 tensor(-107.4864)\n",
      "epoch 73 loss tensor(0.3053, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3314175605773926 tensor(-107.4865)\n",
      "epoch 74 loss tensor(0.0689, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.336590051651001 tensor(-107.4949)\n",
      "epoch 75 loss tensor(0.2620, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3351874351501465 tensor(-107.4921)\n",
      "epoch 76 loss tensor(0.1621, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3396322727203369 tensor(-107.4818)\n",
      "epoch 77 loss tensor(0.4801, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33553266525268555 tensor(-107.4901)\n",
      "epoch 78 loss tensor(0.1260, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3365662097930908 tensor(-107.4806)\n",
      "epoch 79 loss tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33243298530578613 tensor(-107.4855)\n",
      "epoch 80 loss tensor(0.1234, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3354830741882324 tensor(-107.4892)\n",
      "epoch 81 loss tensor(0.1723, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3355085849761963 tensor(-107.4938)\n",
      "epoch 82 loss tensor(0.4199, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3374826908111572 tensor(-107.4917)\n",
      "epoch 83 loss tensor(1.2890, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3343994617462158 tensor(-107.4801)\n",
      "epoch 84 loss tensor(0.2832, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3344392776489258 tensor(-107.4876)\n",
      "epoch 85 loss tensor(0.1047, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.337587833404541 tensor(-107.4904)\n",
      "epoch 86 loss tensor(0.1587, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33718061447143555 tensor(-107.4863)\n",
      "epoch 87 loss tensor(0.0411, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3378458023071289 tensor(-107.4940)\n",
      "epoch 88 loss tensor(0.1840, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3381164073944092 tensor(-107.4886)\n",
      "epoch 89 loss tensor(0.2806, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.34389710426330566 tensor(-107.4939)\n",
      "epoch 90 loss tensor(0.3370, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33844733238220215 tensor(-107.4891)\n",
      "epoch 91 loss tensor(0.0256, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3335142135620117 tensor(-107.4944)\n",
      "epoch 92 loss tensor(0.3605, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3371117115020752 tensor(-107.4916)\n",
      "epoch 93 loss tensor(0.0584, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33399200439453125 tensor(-107.5089)\n",
      "epoch 94 loss tensor(0.1528, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33300161361694336 tensor(-107.4862)\n",
      "epoch 95 loss tensor(0.6691, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3357408046722412 tensor(-107.4953)\n",
      "epoch 96 loss tensor(0.1439, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3366849422454834 tensor(-107.5091)\n",
      "epoch 97 loss tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33545470237731934 tensor(-107.5071)\n",
      "epoch 98 loss tensor(0.1252, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33832645416259766 tensor(-107.4973)\n",
      "epoch 99 loss tensor(0.0373, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3357203006744385 tensor(-107.4987)\n",
      "Log saved at: experiments_logs/gqe_sweep/temperature_1.0/metrics.csv\n",
      "\n",
      "Running with temperature = 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 86.92M\n",
      "epoch 0 loss tensor(0.4556, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.34794020652770996 tensor(-107.4851)\n",
      "epoch 1 loss tensor(589.6943, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3389749526977539 tensor(-107.4254)\n",
      "epoch 2 loss tensor(1.9331, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3418464660644531 tensor(-107.4511)\n",
      "epoch 3 loss tensor(1.7309, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3381693363189697 tensor(-107.4873)\n",
      "epoch 4 loss tensor(28199.1348, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.34473133087158203 tensor(-107.3943)\n",
      "epoch 5 loss tensor(10481.5342, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3391120433807373 tensor(-107.3077)\n",
      "epoch 6 loss tensor(124.3686, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32923388481140137 tensor(-107.4735)\n",
      "epoch 7 loss tensor(175.8037, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3335845470428467 tensor(-107.4235)\n",
      "epoch 8 loss tensor(0.7377, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3274228572845459 tensor(-107.4839)\n",
      "epoch 9 loss tensor(0.0907, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33119988441467285 tensor(-107.4813)\n",
      "epoch 10 loss tensor(1.3249, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33023905754089355 tensor(-107.4854)\n",
      "epoch 11 loss tensor(8.9695, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3252565860748291 tensor(-107.4884)\n",
      "epoch 12 loss tensor(1.7324, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3241298198699951 tensor(-107.4899)\n",
      "epoch 13 loss tensor(0.1450, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3245677947998047 tensor(-107.4918)\n",
      "epoch 14 loss tensor(0.7807, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32227063179016113 tensor(-107.4813)\n",
      "epoch 15 loss tensor(0.2758, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32272911071777344 tensor(-107.4854)\n",
      "epoch 16 loss tensor(0.0961, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3267643451690674 tensor(-107.4963)\n",
      "epoch 17 loss tensor(1.8676, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3323538303375244 tensor(-107.4935)\n",
      "epoch 18 loss tensor(1.1737, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32276487350463867 tensor(-107.4919)\n",
      "epoch 19 loss tensor(0.1042, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3232717514038086 tensor(-107.4894)\n",
      "epoch 20 loss tensor(0.3017, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3242487907409668 tensor(-107.4853)\n",
      "epoch 21 loss tensor(0.2174, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3284440040588379 tensor(-107.4741)\n",
      "epoch 22 loss tensor(0.3375, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33232760429382324 tensor(-107.4849)\n",
      "epoch 23 loss tensor(0.1094, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33231163024902344 tensor(-107.4850)\n",
      "epoch 24 loss tensor(8.5590, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32265663146972656 tensor(-107.4874)\n",
      "epoch 25 loss tensor(6.9664, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3218240737915039 tensor(-107.4828)\n",
      "epoch 26 loss tensor(4.7550, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32962942123413086 tensor(-107.4815)\n",
      "epoch 27 loss tensor(2.5810, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32573485374450684 tensor(-107.4917)\n",
      "epoch 28 loss tensor(1.3655, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3334178924560547 tensor(-107.4697)\n",
      "epoch 29 loss tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3331332206726074 tensor(-107.5029)\n",
      "epoch 30 loss tensor(0.1925, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33089160919189453 tensor(-107.5011)\n",
      "epoch 31 loss tensor(1.9584, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3318791389465332 tensor(-107.4927)\n",
      "epoch 32 loss tensor(1.8557, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33673882484436035 tensor(-107.5059)\n",
      "epoch 33 loss tensor(0.4055, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32706689834594727 tensor(-107.5196)\n",
      "epoch 34 loss tensor(0.0166, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3271522521972656 tensor(-107.5168)\n",
      "epoch 35 loss tensor(0.1914, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3296539783477783 tensor(-107.5189)\n",
      "epoch 36 loss tensor(0.0255, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3300666809082031 tensor(-107.5017)\n",
      "epoch 37 loss tensor(0.3787, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32976460456848145 tensor(-107.5066)\n",
      "epoch 38 loss tensor(0.1404, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3287951946258545 tensor(-107.5206)\n",
      "epoch 39 loss tensor(0.0799, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3254730701446533 tensor(-107.4911)\n",
      "epoch 40 loss tensor(1.1048, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3284153938293457 tensor(-107.4677)\n",
      "epoch 41 loss tensor(0.7845, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3284749984741211 tensor(-107.4650)\n",
      "epoch 42 loss tensor(0.0431, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33151721954345703 tensor(-107.4783)\n",
      "epoch 43 loss tensor(0.3857, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.325603723526001 tensor(-107.4887)\n",
      "epoch 44 loss tensor(0.0816, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3237614631652832 tensor(-107.4887)\n",
      "epoch 45 loss tensor(0.0928, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3240635395050049 tensor(-107.4751)\n",
      "epoch 46 loss tensor(0.0424, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3324723243713379 tensor(-107.4823)\n",
      "epoch 47 loss tensor(1.9743, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32502031326293945 tensor(-107.4476)\n",
      "epoch 48 loss tensor(1.8572, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33113741874694824 tensor(-107.4817)\n",
      "epoch 49 loss tensor(0.4621, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32532644271850586 tensor(-107.4847)\n",
      "epoch 50 loss tensor(0.0695, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32822322845458984 tensor(-107.4564)\n",
      "epoch 51 loss tensor(0.1417, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32501721382141113 tensor(-107.4773)\n",
      "epoch 52 loss tensor(0.0769, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33533596992492676 tensor(-107.4899)\n",
      "epoch 53 loss tensor(0.1160, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.35559606552124023 tensor(-107.4770)\n",
      "epoch 54 loss tensor(0.1025, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32445549964904785 tensor(-107.4757)\n",
      "epoch 55 loss tensor(0.0220, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3275165557861328 tensor(-107.4912)\n",
      "epoch 56 loss tensor(0.0946, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3262481689453125 tensor(-107.4914)\n",
      "epoch 57 loss tensor(2.5253, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.321868896484375 tensor(-107.4934)\n",
      "epoch 58 loss tensor(1.9754, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3246304988861084 tensor(-107.4934)\n",
      "epoch 59 loss tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3288261890411377 tensor(-107.4895)\n",
      "epoch 60 loss tensor(0.3313, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3317906856536865 tensor(-107.4923)\n",
      "epoch 61 loss tensor(0.0524, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3342249393463135 tensor(-107.4864)\n",
      "epoch 62 loss tensor(2.9037, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33371591567993164 tensor(-107.4852)\n",
      "epoch 63 loss tensor(2.8747, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33461451530456543 tensor(-107.4906)\n",
      "epoch 64 loss tensor(1.4815, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32949256896972656 tensor(-107.4744)\n",
      "epoch 65 loss tensor(0.4748, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3274807929992676 tensor(-107.4781)\n",
      "epoch 66 loss tensor(0.1187, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32942962646484375 tensor(-107.4662)\n",
      "epoch 67 loss tensor(0.0942, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32875537872314453 tensor(-107.4878)\n",
      "epoch 68 loss tensor(0.1379, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3308999538421631 tensor(-107.4801)\n",
      "epoch 69 loss tensor(0.0857, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32781076431274414 tensor(-107.4842)\n",
      "epoch 70 loss tensor(0.1118, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3284945487976074 tensor(-107.4914)\n",
      "epoch 71 loss tensor(0.3868, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32640600204467773 tensor(-107.4932)\n",
      "epoch 72 loss tensor(0.1896, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3269944190979004 tensor(-107.4931)\n",
      "epoch 73 loss tensor(0.2454, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3287367820739746 tensor(-107.4938)\n",
      "epoch 74 loss tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.335437536239624 tensor(-107.4977)\n",
      "epoch 75 loss tensor(3.9027, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.36218690872192383 tensor(-107.4971)\n",
      "epoch 76 loss tensor(4.1301, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3315136432647705 tensor(-107.4972)\n",
      "epoch 77 loss tensor(1.5399, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3228476047515869 tensor(-107.4933)\n",
      "epoch 78 loss tensor(0.3795, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32251572608947754 tensor(-107.4897)\n",
      "epoch 79 loss tensor(0.0161, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3268311023712158 tensor(-107.4763)\n",
      "epoch 80 loss tensor(0.2652, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32924413681030273 tensor(-107.4824)\n",
      "epoch 81 loss tensor(0.0679, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3309628963470459 tensor(-107.4902)\n",
      "epoch 82 loss tensor(2.1297, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33165454864501953 tensor(-107.5086)\n",
      "epoch 83 loss tensor(2.3914, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3330204486846924 tensor(-107.5073)\n",
      "epoch 84 loss tensor(1.1849, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.329892635345459 tensor(-107.4926)\n",
      "epoch 85 loss tensor(0.5057, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33016180992126465 tensor(-107.4820)\n",
      "epoch 86 loss tensor(0.0310, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3310813903808594 tensor(-107.4792)\n",
      "epoch 87 loss tensor(0.0586, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32375311851501465 tensor(-107.4786)\n",
      "epoch 88 loss tensor(0.0233, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3228299617767334 tensor(-107.4850)\n",
      "epoch 89 loss tensor(0.1040, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.325897216796875 tensor(-107.4906)\n",
      "epoch 90 loss tensor(1.8218, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3318665027618408 tensor(-107.4660)\n",
      "epoch 91 loss tensor(1.0173, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32938265800476074 tensor(-107.4760)\n",
      "epoch 92 loss tensor(0.0627, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32905006408691406 tensor(-107.4828)\n",
      "epoch 93 loss tensor(0.1021, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3259847164154053 tensor(-107.4974)\n",
      "epoch 94 loss tensor(0.1908, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3303098678588867 tensor(-107.4946)\n",
      "epoch 95 loss tensor(0.2167, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.32850122451782227 tensor(-107.4926)\n",
      "epoch 96 loss tensor(0.0385, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3271365165710449 tensor(-107.4936)\n",
      "epoch 97 loss tensor(0.0408, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33136844635009766 tensor(-107.4883)\n",
      "epoch 98 loss tensor(0.0666, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.33241868019104004 tensor(-107.4933)\n",
      "epoch 99 loss tensor(0.5466, device='cuda:0', grad_fn=<MseLossBackward0>) model.train_step time: 0.3298931121826172 tensor(-107.4945)\n",
      "Log saved at: experiments_logs/gqe_sweep/temperature_5.0/metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2519516/3677365917.py:75: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Ë®≠ÂÆöÂØ¶È©óÂèÉÊï∏ ===\n",
    "temp_values = [0.1, 1.0, 5.0]\n",
    "\n",
    "# ÊÅ¢Âæ©È†êË®≠ LR\n",
    "cfg.lr = 1e-4\n",
    "cfg.max_iters = 100\n",
    "\n",
    "# === Âü∑Ë°åÊéÉÊèè ===\n",
    "temp_logs = run_parameter_sweep('temperature', temp_values)\n",
    "\n",
    "# === Áï´Âúñ ===\n",
    "plot_loss_curves(temp_logs, 'Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9e331-6c4a-4e1e-8adb-3a95aaab0a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733380f-286d-47c3-b8e0-9aa689919318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27863420-beca-4d06-93c4-0630456846cf",
   "metadata": {},
   "source": [
    "## SQD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "84a3be6a-daa9-4882-881a-53467991972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Ground Energy: -107.51065919192828\n",
      "{'11001111111001': 1, '11110011111001': 1, '11110110111001': 4, '11111011011001': 4, '10111101110110': 4, '11111100110110': 4, '11110101111010': 90, '11101111010011': 2, '11111011110100': 26, '11011101111010': 9, '11111111110000': 9855}\n"
     ]
    }
   ],
   "source": [
    "if not args.mpi or cudaq.mpi.rank() == 0:\n",
    "    final_coeffs = []\n",
    "    final_words = []\n",
    "    for idx in best_ops:\n",
    "        op = op_pool[idx]\n",
    "        final_coeffs += [c.real for c in term_coefficients(op)]\n",
    "        final_words += term_words(op)\n",
    "\n",
    "    energy_result = cudaq.observe(kernel, spin_ham, n_qubits, n_electrons, final_coeffs, final_words)\n",
    "    ground_energy = energy_result.expectation()\n",
    "    # total_energy = ground_energy + nuclear_repulsion_energy\n",
    "    \n",
    "    print(f\"Calculated Ground Energy: {ground_energy}\")\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # 2. Âü∑Ë°åÊé°Ê®£ (ÁÇ∫‰∫ÜÁµ¶ SQD Áî®)\n",
    "    sample_result = cudaq.sample(kernel, n_qubits, n_electrons, final_coeffs, final_words, shots_count=10000)\n",
    "    config_counts = dict(sample_result.items())\n",
    "\n",
    "    print(config_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dd666744-847b-4005-9f41-f848165b4930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: {'11001111111001': 1, '11110011111001': 1, '11110110111001': 4, '11111011011001': 4, '10111101110110': 4, '11111100110110': 4, '11110101111010': 90, '11101111010011': 2, '11111011110100': 26, '11011101111010': 9, '11111111110000': 9855}\n",
      "Reordered: {'01111011011101': 1, '01110111011011': 1, '01110111010111': 4, '01011111011011': 4, '10101110111110': 4, '10101110110111': 4, '11100110011111': 90, '10011111011101': 2, '00111110111011': 26, '11101010011111': 9, '00111110011111': 9855}\n",
      "(Format: Left=Down Spins, Right=Up Spins)\n"
     ]
    }
   ],
   "source": [
    "def get_spin_ordered_results(counts):\n",
    "    \"\"\"\n",
    "    ÂÉÖÊîπËÆä Bitstring ÁöÑÈ†ÜÂ∫èÔºå‰∏çÈÄ≤Ë°å‰ªª‰ΩïÈÅéÊøæ„ÄÇ\n",
    "    Ëº∏ÂÖ•Ê†ºÂºè (Big Endian): qN...q0 (ÂÖ∂‰∏≠ q0, q1 ÁÇ∫Á¨¨‰∏ÄÂÄãËªåÂüüÁöÑ alpha, beta)\n",
    "    Ëº∏Âá∫Ê†ºÂºè: \"Down_Part + Up_Part\" (Âç≥ \"1‰∏ã 2‰∏ã ... 1‰∏ä 2‰∏ä ...\")\n",
    "    \"\"\"\n",
    "    ordered_dict = {}\n",
    "    \n",
    "    for config, count in counts.items():\n",
    "        # 1. ÂèçËΩâÂ≠ó‰∏≤‰ª•ÂèñÂæó Little Endian (q0, q1, q2...)\n",
    "        #    q0: Orb 0 Up\n",
    "        #    q1: Orb 0 Down\n",
    "        #    q2: Orb 1 Up\n",
    "        #    q3: Orb 1 Down\n",
    "        rconfig = config[::-1]\n",
    "        \n",
    "        # 2. ÊèêÂèñ Up ÈÉ®ÂàÜ (ÂÅ∂Êï∏Á¥¢Âºï 0, 2, 4...) -> Â∞çÊáâ 1‰∏ä, 2‰∏ä, 3‰∏ä...\n",
    "        up_part = rconfig[0::2]\n",
    "        \n",
    "        # 3. ÊèêÂèñ Down ÈÉ®ÂàÜ (Â•áÊï∏Á¥¢Âºï 1, 3, 5...) -> Â∞çÊáâ 1‰∏ã, 2‰∏ã, 3‰∏ã...\n",
    "        down_part = rconfig[1::2]\n",
    "        \n",
    "        # 4. ÁµÑÂêàÊñ∞ÁöÑ Key (‰øÆÊîπËôïÔºöÂ∑¶ÈÇäÊòØ Down, Âè≥ÈÇäÊòØ Up)\n",
    "        new_key = down_part + up_part\n",
    "        \n",
    "        # 5. Â≠òÂÖ•Â≠óÂÖ∏\n",
    "        ordered_dict[new_key] = count\n",
    "        \n",
    "    return ordered_dict\n",
    "\n",
    "# Ê∏¨Ë©¶Ëº∏Âá∫\n",
    "# ÂÅáË®≠ config_counts Â∑≤Á∂ìÂÆöÁæ©\n",
    "final_conserved_dict = get_spin_ordered_results(config_counts)\n",
    "print(f\"Original: {config_counts}\")\n",
    "print(f\"Reordered: {final_conserved_dict}\")\n",
    "print(\"(Format: Left=Down Spins, Right=Up Spins)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "74188a76-eb93-4c55-abe5-251df1697cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitArray(<shape=(), num_shots=10000, num_bits=14>)\n"
     ]
    }
   ],
   "source": [
    "from qiskit_addon_sqd.counts import BitArray\n",
    "bitstrings = list(final_conserved_dict.keys())\n",
    "bitvalues = [int(b, 2) for b in bitstrings]\n",
    "\n",
    "# Ê†πÊìöÂá∫ÁèæÊ¨°Êï∏ÁîüÊàêÂ∞çÊáâÁöÑÊï∏Êìö\n",
    "samples = []\n",
    "for bitstring, count in final_conserved_dict.items():\n",
    "    samples.extend([int(bitstring, 2)] * count)  # ÈáçË§áÂá∫ÁèæÁöÑÊï∏Â≠ó\n",
    "\n",
    "# ÊâæÂá∫ num_bits\n",
    "num_bits = len(bitstrings[0])  # ÂÅáË®≠ÊâÄÊúâÁöÑ bit Â≠ó‰∏≤Èï∑Â∫¶Áõ∏Âêå\n",
    "\n",
    "# Â∞áÈÄô‰∫õÊï¥Êï∏Êï∏ÊìöÊâìÂåÖÁÇ∫ uint8 Èô£Âàó\n",
    "num_bytes = (num_bits + 7) // 8  # ÊØèÂÄã bit Â≠ó‰∏≤ÁöÑÂ§ßÂ∞èÔºà‰ª•Â≠óÁØÄË®àÁÆóÔºâ\n",
    "\n",
    "# ËΩâÊèõÁÇ∫‰∫åÈÄ≤‰ΩçÂ≠óÁØÄ‰∏≤\n",
    "data = b\"\".join(val.to_bytes(num_bytes, \"big\") for val in samples)\n",
    "array = np.frombuffer(data, dtype=np.uint8)\n",
    "\n",
    "# ÂÅáË®≠‰Ω†Â∑≤Á∂ìÊúâ BitArray È°ûÂà•ÔºàÂ¶ÇÂâçÈù¢Êèê‰æõÁöÑ‰ª£Á¢ºÔºâ\n",
    "# ‰Ω†ÂèØ‰ª•Â∞áÈÄôÂÄãÊï∏ÊìöËΩâÊèõÁÇ∫ BitArray È°ûÂûã\n",
    "bit_array = BitArray(array.reshape(-1, num_bytes), num_bits=num_bits)\n",
    "\n",
    "# Ëº∏Âá∫ÁµêÊûú\n",
    "print(bit_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "315620d9-3669-47b5-9370-8c024ff45b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "\tSubsample 0\n",
      "\t\tEnergy: -29.96039077355908\n",
      "\t\tSubspace dimension: 36\n",
      "\tSubsample 1\n",
      "\t\tEnergy: -29.96039077355908\n",
      "\t\tSubspace dimension: 36\n",
      "\tSubsample 2\n",
      "\t\tEnergy: -29.96039077355908\n",
      "\t\tSubspace dimension: 36\n",
      "Iteration 2\n",
      "\tSubsample 0\n",
      "\t\tEnergy: -30.671133202052054\n",
      "\t\tSubspace dimension: 81\n",
      "\tSubsample 1\n",
      "\t\tEnergy: -30.671133202052054\n",
      "\t\tSubspace dimension: 81\n",
      "\tSubsample 2\n",
      "\t\tEnergy: -30.671133202052054\n",
      "\t\tSubspace dimension: 81\n",
      "Iteration 3\n",
      "\tSubsample 0\n",
      "\t\tEnergy: -30.67781262445968\n",
      "\t\tSubspace dimension: 169\n",
      "\tSubsample 1\n",
      "\t\tEnergy: -30.67781262445968\n",
      "\t\tSubspace dimension: 169\n",
      "\tSubsample 2\n",
      "\t\tEnergy: -30.67781262445968\n",
      "\t\tSubspace dimension: 169\n",
      "Iteration 4\n",
      "\tSubsample 0\n",
      "\t\tEnergy: -30.67781262445968\n",
      "\t\tSubspace dimension: 169\n",
      "\tSubsample 1\n",
      "\t\tEnergy: -30.67781262445968\n",
      "\t\tSubspace dimension: 169\n",
      "\tSubsample 2\n",
      "\t\tEnergy: -30.67781262445968\n",
      "\t\tSubspace dimension: 169\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    " \n",
    "from qiskit_addon_sqd.fermion import (\n",
    "    SCIResult,\n",
    "    diagonalize_fermionic_hamiltonian,\n",
    "    solve_sci_batch,\n",
    ")\n",
    " \n",
    "# SQD options\n",
    "energy_tol = 1e-6\n",
    "occupancies_tol = 1e-6\n",
    "max_iterations = 5\n",
    " \n",
    "# Eigenstate solver options\n",
    "num_batches = 3\n",
    "samples_per_batch = 100\n",
    "symmetrize_spin = True\n",
    "carryover_threshold = 1e-4\n",
    "max_cycle = 200\n",
    " \n",
    "# Pass options to the built-in eigensolver. If you just want to use the defaults,\n",
    "# you can omit this step, in which case you would not specify the sci_solver argument\n",
    "# in the call to diagonalize_fermionic_hamiltonian below.\n",
    "sci_solver = partial(solve_sci_batch, spin_sq=0.0, max_cycle=max_cycle)\n",
    " \n",
    "# List to capture intermediate results\n",
    "result_history = []\n",
    " \n",
    " \n",
    "def callback(results: list[SCIResult]):\n",
    "    result_history.append(results)\n",
    "    iteration = len(result_history)\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\tSubsample {i}\")\n",
    "        print(f\"\\t\\tEnergy: {result.energy + nuclear_repulsion_energy}\")\n",
    "        print(\n",
    "            f\"\\t\\tSubspace dimension: {np.prod(result.sci_state.amplitudes.shape)}\"\n",
    "        )\n",
    " \n",
    " \n",
    "result = diagonalize_fermionic_hamiltonian(\n",
    "    hcore,\n",
    "    eri,\n",
    "    bit_array,\n",
    "    samples_per_batch=samples_per_batch,\n",
    "    norb=norb_cas,\n",
    "    nelec=nele_cas_tuple,\n",
    "    num_batches=num_batches,\n",
    "    energy_tol=energy_tol,\n",
    "    occupancies_tol=occupancies_tol,\n",
    "    max_iterations=max_iterations,\n",
    "    sci_solver=sci_solver,\n",
    "    symmetrize_spin=symmetrize_spin,\n",
    "    carryover_threshold=carryover_threshold,\n",
    "    callback=callback,\n",
    "    seed=12345,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cudaq-env)",
   "language": "python",
   "name": "cudaq-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
